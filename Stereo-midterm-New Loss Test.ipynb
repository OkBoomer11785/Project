{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11785 - Midterm Submission\n",
    "\n",
    "The following code is for training a self supervised model that predicts depth from images, where training is done using stereo pairs from the KITTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  \n",
    "\n",
    "import PIL.Image as pil\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import autograd\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "## Some helper functions are used from the original implementation\n",
    "from utils import *\n",
    "from kitti_utils import *\n",
    "from layers import *\n",
    "\n",
    "\n",
    "import datasets\n",
    "from depthDecoder import *\n",
    "# import networks\n",
    "from IPython import embed\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv3x3, self).__init__()\n",
    "\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = Conv3x3(in_channels, out_channels)\n",
    "        self.nonlin = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.nonlin(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.use_skips = use_skips\n",
    "        self.scales = scales\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
    "\n",
    "        self.convs = OrderedDict()\n",
    "        for i in range(4, -1, -1):\n",
    "            # upconv_0\n",
    "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "        \n",
    "            num_ch_in = self.num_ch_dec[i]\n",
    "            if self.use_skips and i > 0:\n",
    "                num_ch_in += self.num_ch_enc[i - 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        for s in self.scales:\n",
    "            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
    "\n",
    "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        self.outputs = {}\n",
    "\n",
    "        # decoder\n",
    "        x = input_features[-1]\n",
    "        for i in range(4, -1, -1):\n",
    "            x = self.convs[(\"upconv\", i, 0)](x)\n",
    "            x = [F.interpolate(x, scale_factor=2, mode=\"nearest\")]\n",
    "            if self.use_skips and i > 0:\n",
    "                x += [input_features[i - 1]]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.convs[(\"upconv\", i, 1)](x)\n",
    "            if i in self.scales:\n",
    "                self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n",
    "\n",
    "        return self.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, input_channel, channel, kernel_size, stride_val):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channel, channel, kernel_size, stride_val,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel_size, stride=1, padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channel)\n",
    "        \n",
    "        self.conv3_sc = torch.nn.Conv2d(input_channel,channel,kernel_size=1,stride=stride_val,bias = False)\n",
    "        self.bn3_sc = torch.nn.BatchNorm2d(channel)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channel, channel, kernel_size, stride=1,padding=1,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channel)\n",
    "        self.conv4 = nn.Conv2d(channel, channel, kernel_size, stride=1,padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(channel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        identity = self.bn3_sc(self.conv3_sc(identity))\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        identity1 = out\n",
    "        out1 = F.relu(self.bn3(self.conv3(out)))\n",
    "        out1 = self.bn4(self.conv3(out1))\n",
    "        out1 += identity1\n",
    "        out1 = F.relu(out1)\n",
    "        return out     \n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "        \n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = ResBlock(input_channel = 64, channel = 64, kernel_size = 3, stride_val = 1)\n",
    "        self.layer2 = ResBlock(input_channel = 64, channel = 128, kernel_size = 3, stride_val = 2)\n",
    "        self.layer3 = ResBlock(input_channel = 128, channel = 256, kernel_size = 3, stride_val = 2)\n",
    "        self.layer4 = ResBlock(input_channel = 256, channel = 512, kernel_size = 3, stride_val = 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        self.features.append(self.relu(x))\n",
    "        self.features.append(self.layer1(self.maxpool(self.features[-1])))\n",
    "        self.features.append(self.layer2(self.features[-1]))\n",
    "        self.features.append(self.layer3(self.features[-1]))\n",
    "        self.features.append(self.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,data_path,filenames,height,width,frame_idxs,num_scales,is_train=False,img_ext='.jpg'):\n",
    "        super(MainDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.filenames = filenames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_scales = num_scales\n",
    "        self.frame_idxs = frame_idxs\n",
    "        self.is_train = is_train\n",
    "        self.img_ext = img_ext\n",
    "        \n",
    "        self.interp = Image.ANTIALIAS\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        try:\n",
    "            self.brightness = (0.8, 1.2)\n",
    "            self.contrast = (0.8, 1.2)\n",
    "            self.saturation = (0.8, 1.2)\n",
    "            self.hue = (-0.1, 0.1)\n",
    "            transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        except TypeError:\n",
    "            self.brightness = 0.2\n",
    "            self.contrast = 0.2\n",
    "            self.saturation = 0.2\n",
    "            self.hue = 0.1\n",
    "\n",
    "               \n",
    "        self.resize = {}\n",
    "        for i in range(self.num_scales):\n",
    "            s = 2 ** i\n",
    "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),interpolation=self.interp)\n",
    "            \n",
    "        self.load_depth = self.check_depth()\n",
    "        \n",
    "        self.K = np.array([[0.58, 0, 0.5, 0],\n",
    "                           [0, 1.92, 0.5, 0],\n",
    "                           [0, 0, 1, 0],\n",
    "                           [0, 0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "        self.full_res_shape = (1242, 375)\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def preprocess(self, inputs, color_aug):\n",
    "        \n",
    "        for k in list(inputs):\n",
    "            frame = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                for i in range(self.num_scales):\n",
    "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
    "\n",
    "        for k in list(inputs):\n",
    "            f = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                inputs[(n, im, i)] = self.to_tensor(f)\n",
    "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
    "                \n",
    "    \n",
    "\n",
    "                    \n",
    "    def check_depth(self):\n",
    "        line = self.filenames[0].split()\n",
    "        scene_name = line[0]\n",
    "        frame_index = int(line[1])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            scene_name,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        return os.path.isfile(velo_filename)\n",
    "    \n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "    #def get_color(self, folder, frame_index, side):\n",
    "        \n",
    "        color = self.loader(self.get_image_path(folder, frame_index, side))\n",
    "        \n",
    "        if do_flip:\n",
    "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return color\n",
    "    \n",
    "    def get_image_path(self, folder, frame_index, side):\n",
    "        f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\n",
    "        image_path = os.path.join(\n",
    "            self.data_path, folder, \"image_0{}/data\".format(self.side_map[side]), f_str)\n",
    "        return image_path\n",
    "\n",
    "    def get_depth(self, folder, frame_index, side):\n",
    "        calib_path = os.path.join(self.data_path, folder.split(\"/\")[0])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            folder,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\n",
    "        depth_gt = skimage.transform.resize(\n",
    "            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\n",
    "\n",
    "\n",
    "        return depth_gt\n",
    "    \n",
    "\n",
    "                    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        inputs = {}\n",
    "        \n",
    "        do_color_aug = self.is_train and random.random() > 0.5\n",
    "        do_flip = self.is_train and random.random() > 0.5\n",
    "        \n",
    "        line = self.filenames[index].split()\n",
    "        folder = line[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(line) == 3:\n",
    "            frame_index = int(line[1])\n",
    "            side = line[2]\n",
    "        else:\n",
    "            frame_index = 0\n",
    "            side = None\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            if i == \"s\":\n",
    "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\n",
    "                #inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side)\n",
    "            else:\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n",
    "                #inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side)\n",
    "                \n",
    "        for scale in range(self.num_scales):\n",
    "            K = self.K.copy()\n",
    "\n",
    "            K[0, :] *= self.width // (2 ** scale)\n",
    "            K[1, :] *= self.height // (2 ** scale)\n",
    "\n",
    "            inv_K = np.linalg.pinv(K)\n",
    "\n",
    "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
    "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
    "\n",
    "        if do_color_aug:\n",
    "            color_aug = transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        else:\n",
    "            color_aug =  (lambda x: x)\n",
    "            \n",
    "        self.preprocess(inputs,color_aug)\n",
    "        \n",
    "        for i in self.frame_idxs:\n",
    "            del inputs[(\"color\", i, -1)]\n",
    "            del inputs[(\"color_aug\", i, -1)]\n",
    "            \n",
    "        if self.load_depth:\n",
    "            depth_gt = self.get_depth(folder, frame_index, side)\n",
    "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
    "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
    "            \n",
    "        if \"s\" in self.frame_idxs:\n",
    "            stereo_T = np.eye(4, dtype=np.float32)\n",
    "            baseline_sign =  +1 #-1 if do_flip else 1\n",
    "            side_sign = -1 if side == \"l\" else 1\n",
    "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n",
    "\n",
    "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data according to files listed in eigen_full split \n",
    "\n",
    "dataset = datasets.KITTIRAWDataset\n",
    "data_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "fpath_train = \"/home/ubuntu/monodepth2/splits/eigen_full/train_files.txt\"\n",
    "f_train = open(fpath_train)\n",
    "train_filenames = f_train.readlines()\n",
    "fpath_val = \"/home/ubuntu/monodepth2/splits/eigen_full/val_files.txt\"\n",
    "f_val = open(fpath_val)\n",
    "val_filenames = f_val.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Size to rescale all input images to\n",
    "height = 192\n",
    "width = 640\n",
    "\n",
    "## Scales of the output image from each layer of the decoder\n",
    "## output scale = input scale/2**(n) for n in scales\n",
    "scales = np.arange(4)\n",
    "\n",
    "img_ext = '.jpg'\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "## List of frames that the dataloader must return \n",
    "## 0 - present frame\n",
    "## 's' - stereo couple frame\n",
    "frame_ids = [0,'s']\n",
    "\n",
    "\n",
    "\n",
    "weights_pretrained = True\n",
    "\n",
    "num_scales = len(scales)\n",
    "num_input_frames = len(frame_ids)\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_workers = 20\n",
    "\n",
    "train_dataset = dataset(data_path, train_filenames, height, width,frame_ids,num_scales,is_train=True, img_ext=img_ext)\n",
    "train_loader = DataLoader(train_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = dataset(data_path, val_filenames, height, width,frame_ids, 4, is_train=False, img_ext=img_ext)\n",
    "val_loader = DataLoader(val_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthEncoderModule(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DepthEncoderModule, self).__init__()\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "        pretrained = True\n",
    "        self.encoder = models.resnet34(pretrained)\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "## Initialize the models\n",
    "\n",
    "# encoder = ResnetEncoder()\n",
    "encoder = DepthEncoderModule(layers = 34)\n",
    "params += list(encoder.parameters())\n",
    "\n",
    "\n",
    "decoder = Decoder(encoder.num_ch_enc, scales)\n",
    "# decoder = Decoder(encoder.num_ch_enc, scales)\n",
    "params += list(decoder.parameters())\n",
    "\n",
    "\n",
    "ssim = SSIM()\n",
    "ssim.to(device)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "optimizer = optim.Adam(params, learning_rate)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 15, 0.1)\n",
    "\n",
    "depth_metric_names = [\n",
    "            \"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "backproject_depth = {}\n",
    "project_3d = {}\n",
    "for scale in scales:\n",
    "    h = height // (2 ** scale)\n",
    "    w = width // (2 ** scale)\n",
    "\n",
    "    backproject_depth[scale] = BackprojectDepth(batch_size, h, w)\n",
    "    backproject_depth[scale].to(device)\n",
    "\n",
    "    project_3d[scale] = Project3D(batch_size, h, w)\n",
    "    project_3d[scale].to(device)\n",
    "#print(\"There are {:d} training items and {:d} validation items\\n\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images_pred(inputs, outputs):\n",
    "        \n",
    "   \n",
    "    source_scale = 0\n",
    "    \n",
    "    for scale in scales:\n",
    "        disp = outputs[(\"disp\", scale)]\n",
    "\n",
    "        disp = F.interpolate(disp, [height, width], mode=\"bilinear\",align_corners=False)\n",
    "        \n",
    "        depth = 1 / (0.01 + 9.99*disp)\n",
    " \n",
    "        outputs[(\"depth\", 0, scale)] = depth\n",
    "\n",
    "        T = inputs[\"stereo_T\"]\n",
    "\n",
    "        cam_points = backproject_depth[source_scale](depth, inputs[(\"inv_K\", source_scale)])\n",
    "        pix_coords = project_3d[source_scale](cam_points, inputs[(\"K\", source_scale)], T)\n",
    "        outputs[(\"color\", 's', scale)] = F.grid_sample(inputs[(\"color\", 's', source_scale)],\n",
    "                pix_coords,\n",
    "                padding_mode=\"border\",align_corners=False)\n",
    "\n",
    "    return outputs   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import exp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)*2/float(2*sigma*2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim_new(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_reprojection_loss(pred, target):\n",
    "\n",
    "        l1_loss = (torch.abs(target - pred)).mean(1,True)\n",
    "#         l2_loss = torch.sqrt(((target - pred)**2).mean(1, True))\n",
    "        \n",
    "        #ssim_loss = ssim(pred, target).mean(1, True)\n",
    "        ssim_loss = torch.clamp((1 - ssim_new(pred, target,1)) * 0.5, 0, 1)\n",
    "        reprojection_loss = 0.85*ssim_loss + 0.15 * l1_loss\n",
    "#         print('reprojection ', reprojection_loss)\n",
    "        return reprojection_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_loss(disp, img):\n",
    "\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x *= torch.exp(-grad_img_x)\n",
    "    grad_disp_y *= torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_losses(inputs, outputs):\n",
    "\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "        \n",
    "\n",
    "        for scale in scales:\n",
    "            loss = 0\n",
    "            reprojection_losses = []\n",
    "\n",
    "            source_scale = 0\n",
    "\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, source_scale)]\n",
    "\n",
    "\n",
    "            pred = outputs[(\"color\", 's', scale)]\n",
    "            reprojection_losses.append(compute_reprojection_loss(pred, target))\n",
    "\n",
    "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
    "            to_optimise, idxs = torch.min(reprojection_losses, dim=1)\n",
    "            #to_optimise = reprojection_losses\n",
    "            loss = loss + to_optimise.mean()\n",
    "\n",
    "            norm_disp = disp / (disp.mean(2, True).mean(3, True)+ 1e-7)\n",
    "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
    "\n",
    "            loss =  loss + ((1e-3)*smooth_loss) / (2 ** scale)\n",
    "            total_loss = total_loss + loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "\n",
    "        total_loss = total_loss/num_scales\n",
    "        losses[\"loss\"] = total_loss\n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_depth_losses(inputs, outputs, losses):\n",
    "\n",
    "\n",
    "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
    "        depth_pred = torch.clamp(F.interpolate(\n",
    "            depth_pred, [375, 1242], mode=\"bilinear\", align_corners=False), min=1e-3, max=80)\n",
    "        depth_pred = depth_pred.detach()\n",
    "\n",
    "        depth_gt = inputs[\"depth_gt\"]\n",
    "        mask = depth_gt > 0\n",
    "\n",
    "        crop_mask = torch.zeros_like(mask)\n",
    "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * crop_mask\n",
    "\n",
    "        gt = depth_gt[mask]\n",
    "        pred = depth_pred[mask]\n",
    "        pred = pred*(torch.median(gt) / torch.median(pred))\n",
    "\n",
    "        pred = torch.clamp(pred, min=1e-3, max=80)\n",
    "        thresh = torch.max((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).float().mean()\n",
    "        a2 = (thresh < 1.25 ** 2).float().mean()\n",
    "        a3 = (thresh < 1.25 ** 3).float().mean()\n",
    "\n",
    "        rmse = torch.sqrt(((gt - pred)**2).mean())\n",
    "        rmse_log = torch.sqrt(((torch.log(gt + 1e-6) - torch.log(pred + 1e-6))**2).mean())\n",
    "        abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
    "        sq_rel = torch.mean(((gt - pred)**2 )/ gt)\n",
    "        \n",
    "        return np.array([abs_rel.item(),sq_rel.item(),rmse.item(),rmse_log.item(),a1.item(),a2.item(),a3.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader,val_loader,num_epochs):\n",
    "              \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for epoch in range(0,num_epochs):\n",
    "            \n",
    "            total_loss = 0 \n",
    "            outer = tqdm(total=(len(train_loader)), desc='Training Epoch', position=0)\n",
    "\n",
    "            for batch_idx, inputs in enumerate(train_loader):\n",
    "\n",
    "                outer.update(1)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputsl = decoder(features)\n",
    "\n",
    "       \n",
    "                outputs = generate_images_pred(inputs, outputsl)\n",
    "                \n",
    "\n",
    "                losses = compute_losses(inputs, outputs)\n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                del inputs, outputs, features\n",
    "                if batch_idx%500==0 and batch_idx>0:\n",
    "                    print('Batch No: ',batch_idx)\n",
    "                    #print('Loss: ',total_loss/(batch_idx+1))\n",
    "                    print('Loss: ',losses[\"loss\"].cpu().data)\n",
    "                del losses\n",
    "                    \n",
    "  \n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            print('Train Loss at Epoch_{}:'.format(epoch+1),total_loss)\n",
    "            validation(encoder,decoder,val_loader)\n",
    "            run_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,val_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_loss = 0\n",
    "        metrics = np.zeros((7))\n",
    "        for batch_idx, inputs in enumerate(val_loader):\n",
    "\n",
    "            before_op_time = time.time()\n",
    "\n",
    "            for key, ipt in inputs.items():\n",
    "                inputs[key] = ipt.to(device)\n",
    "\n",
    "            features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "            outputs = decoder(features)\n",
    "            \n",
    "            outputs = generate_images_pred(inputs, outputs)\n",
    "            losses = compute_losses(inputs, outputs)\n",
    "            total_loss = total_loss + losses['loss'].item()\n",
    "         \n",
    "            metrics = metrics + compute_depth_losses(inputs, outputs, losses)\n",
    "            del inputs, outputs, losses,features\n",
    "         \n",
    "        total_loss = total_loss/len(val_loader)\n",
    "        print('Total Loss (Validation): ',total_loss)\n",
    "        print('Error Metrics: ',metrics/len(val_loader))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  44%|████▍     | 502/1130 [11:42<35:26,  3.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  500\n",
      "Loss:  tensor(0.3007)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  89%|████████▊ | 1002/1130 [22:39<06:52,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.3059)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1130/1130 [25:04<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_1: 0.30285951938249367\n",
      "Total Loss (Validation):  0.30100185153159226\n",
      "Error Metrics:  [ 0.48467822  5.08808824 12.44722319  0.62208502  0.27230702  0.52409697\n",
      "  0.75657267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type DepthEncoderModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ConvBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Conv3x3. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "Training Epoch:   7%|▋         | 81/1130 [02:09<39:57,  2.29s/it]  "
     ]
    }
   ],
   "source": [
    "num_epochs = 7\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_images(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 7\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'ModifiedL2Loss_Stereo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(epoch_num):\n",
    "    torch.save(encoder,'StereoOutput/Models/Resnet18/'+folder_name+'/DepthEncoder_'+str(epoch_num)+'.pt')\n",
    "    torch.save(decoder,'StereoOutput/Models/Resnet18/'+folder_name+'/DepthDecoder_'+str(epoch_num)+'.pt')\n",
    "    #torch.save(pose_enc,'StereoOutput/Models/Resnet18/'+folder_name+'/PoseEncoder_'+str(epoch_num)+'.pt')\n",
    "    #torch.save(pose_dec,'StereoOutput/Models/Resnet18/'+folder_name+'/PoseDecoder_'+str(epoch_num)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_images(epoch_num):\n",
    "    directory = 'Iteration '+ str(epoch_num)+'/'\n",
    "    parent_dir = 'StereoOutput/Resnet18/'+folder_name+'/'\n",
    "    path = os.path.join(parent_dir, directory) \n",
    "    os.mkdir(path) \n",
    "    image_output_path = parent_dir+directory\n",
    "    for i in range(1, 9):\n",
    "        image_name = 'Test'+str(i)+'.jpg'\n",
    "        image_path = '/home/ubuntu/monodepth2/assets/'+image_name\n",
    "        input_image = pil.open(image_path).convert('RGB')\n",
    "        original_width, original_height = input_image.size\n",
    "        input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "        input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            features = encoder(input_image_pytorch.to(device))\n",
    "            outputs = decoder(features)\n",
    "\n",
    "        disp = outputs[(\"disp\", 0)]\n",
    "        disp_resized = torch.nn.functional.interpolate(disp,(original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        # Saving colormapped depth image\n",
    "        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "        vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(input_image)\n",
    "        plt.title(\"Input\", fontsize=22)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "        plt.title(\"Disparity prediction\", fontsize=22)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(image_output_path+image_name,dpi = 100)\n",
    "    saveModel(epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_images(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "total_loss = 0\n",
    "train_loader = val_loader\n",
    "for batch_idx, inputs in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "               \n",
    "                o1 = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "                losses = compute_losses(inputs, o1)\n",
    "                \n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                optimizer.zero_grad()\n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                i = i+1\n",
    "                \n",
    "                lr_scheduler.step()\n",
    "                \n",
    "                if i>2:\n",
    "                    break\n",
    "                   \n",
    "                del inputs, outputs, losses,features,o1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[('disp', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/ubuntu/monodepth2/assets/Home_Hero5.jpg\"\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    features = encoder(input_image_pytorch.to(device))\n",
    "    outputs = decoder(features)\n",
    "\n",
    "disp = outputs[(\"disp\", 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "# Saving colormapped depth image\n",
    "disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input\", fontsize=22)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "plt.title(\"Disparity prediction\", fontsize=22)\n",
    "plt.axis('off');\n",
    "# plt.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_dataset[0][('color_aug', 's', 0)]\n",
    "b = val_dataset[0][('color', 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ss(nn.Module):\n",
    "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ss, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.refl(x)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "\n",
    "ftest = \"/home/ubuntu/monodepth2/splits/eigen/test_files.txt\"\n",
    "test_filenames = open(ftest).readlines()\n",
    "\n",
    "\n",
    "for f in test_filenames:\n",
    "    path = f.split(\" \")\n",
    "    \n",
    "    \n",
    "    if path[2]=='l\\n':\n",
    "        image_path = os.path.join(test_path,path[0],\"image_03/data\",path[1])\n",
    "    else:\n",
    "        image_path = os.path.join(test_path,path[0],\"image_04/data\",path[1])\n",
    "    print(image_path)\n",
    "    input_image = pil.open(image_path+\".jpg\").convert('RGB')\n",
    "    original_width, original_height = input_image.size\n",
    "    input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        features = encoder(input_image_pytorch.to(device))\n",
    "        outputs = decoder(features)\n",
    "\n",
    "    disp = outputs[(\"disp\", 0)]\n",
    "    disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    # Saving colormapped depth image\n",
    "    disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "    vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.imshow(input_image)\n",
    "    plt.title(\"Input\", fontsize=22)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "    plt.title(\"Disparity prediction\", fontsize=22)\n",
    "    plt.axis('off');\n",
    "    \n",
    "    plt.savefig('output_imgs_10epochs/'+path[1]+'.png',dpi = 100)\n",
    "    \n",
    "    plt.close('all')\n",
    "    del outputs,features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/ubuntu/monodepth2/assets/test_image.jpg\"\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder,'encoder10e.pt')\n",
    "torch.save(decoder,'decoder10e.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('encoder10e.pt')\n",
    "decoder = torch.load('decoder10e.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
