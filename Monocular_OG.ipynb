{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monocular attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  \n",
    "\n",
    "import PIL.Image as pil\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "## Some helper functions are used from the original implementation\n",
    "from utils import *\n",
    "from kitti_utils import *\n",
    "from layers import *\n",
    "\n",
    "\n",
    "import datasets\n",
    "import networks\n",
    "from IPython import embed\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv3x3, self).__init__()\n",
    "\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = Conv3x3(in_channels, out_channels)\n",
    "        self.nonlin = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.nonlin(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.use_skips = use_skips\n",
    "        self.scales = scales\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
    "\n",
    "        self.convs = OrderedDict()\n",
    "        for i in range(4, -1, -1):\n",
    "            # upconv_0\n",
    "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "        \n",
    "            num_ch_in = self.num_ch_dec[i]\n",
    "            if self.use_skips and i > 0:\n",
    "                num_ch_in += self.num_ch_enc[i - 1]\n",
    "            num_ch_out = self.num_ch_dec[i]\n",
    "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
    "\n",
    "        for s in self.scales:\n",
    "            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
    "\n",
    "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        self.outputs = {}\n",
    "\n",
    "        # decoder\n",
    "        x = input_features[-1]\n",
    "        for i in range(4, -1, -1):\n",
    "            x = self.convs[(\"upconv\", i, 0)](x)\n",
    "            x = [F.interpolate(x, scale_factor=2, mode=\"nearest\")]\n",
    "            if self.use_skips and i > 0:\n",
    "                x += [input_features[i - 1]]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.convs[(\"upconv\", i, 1)](x)\n",
    "            if i in self.scales:\n",
    "                self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n",
    "\n",
    "        return self.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, input_channel, channel, kernel_size, stride_val):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channel, channel, kernel_size, stride_val,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel_size, stride=1, padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channel)\n",
    "        \n",
    "        self.conv3_sc = torch.nn.Conv2d(input_channel,channel,kernel_size=1,stride=stride_val,bias = False)\n",
    "        self.bn3_sc = torch.nn.BatchNorm2d(channel)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channel, channel, kernel_size, stride=1,padding=1,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channel)\n",
    "        self.conv4 = nn.Conv2d(channel, channel, kernel_size, stride=1,padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(channel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        identity = self.bn3_sc(self.conv3_sc(identity))\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        identity1 = out\n",
    "        out1 = F.relu(self.bn3(self.conv3(out)))\n",
    "        out1 = self.bn4(self.conv3(out1))\n",
    "        out1 += identity1\n",
    "        out1 = F.relu(out1)\n",
    "        return out     \n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "        \n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = ResBlock(input_channel = 64, channel = 64, kernel_size = 3, stride_val = 1)\n",
    "        self.layer2 = ResBlock(input_channel = 64, channel = 128, kernel_size = 3, stride_val = 2)\n",
    "        self.layer3 = ResBlock(input_channel = 128, channel = 256, kernel_size = 3, stride_val = 2)\n",
    "        self.layer4 = ResBlock(input_channel = 256, channel = 512, kernel_size = 3, stride_val = 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        self.features.append(self.relu(x))\n",
    "        self.features.append(self.layer1(self.maxpool(self.features[-1])))\n",
    "        self.features.append(self.layer2(self.features[-1]))\n",
    "        self.features.append(self.layer3(self.features[-1]))\n",
    "        self.features.append(self.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,data_path,filenames,height,width,frame_idxs,num_scales,is_train=False,img_ext='.jpg'):\n",
    "        super(MainDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.filenames = filenames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_scales = num_scales\n",
    "        self.frame_idxs = frame_idxs\n",
    "        self.is_train = is_train\n",
    "        self.img_ext = img_ext\n",
    "        \n",
    "        self.interp = Image.ANTIALIAS\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        try:\n",
    "            self.brightness = (0.8, 1.2)\n",
    "            self.contrast = (0.8, 1.2)\n",
    "            self.saturation = (0.8, 1.2)\n",
    "            self.hue = (-0.1, 0.1)\n",
    "            transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        except TypeError:\n",
    "            self.brightness = 0.2\n",
    "            self.contrast = 0.2\n",
    "            self.saturation = 0.2\n",
    "            self.hue = 0.1\n",
    "\n",
    "               \n",
    "        self.resize = {}\n",
    "        for i in range(self.num_scales):\n",
    "            s = 2 ** i\n",
    "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),interpolation=self.interp)\n",
    "            \n",
    "        self.load_depth = self.check_depth()\n",
    "        \n",
    "        self.K = np.array([[0.58, 0, 0.5, 0],\n",
    "                           [0, 1.92, 0.5, 0],\n",
    "                           [0, 0, 1, 0],\n",
    "                           [0, 0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "        self.full_res_shape = (1242, 375)\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def preprocess(self, inputs, color_aug):\n",
    "        \n",
    "        for k in list(inputs):\n",
    "            frame = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                for i in range(self.num_scales):\n",
    "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
    "\n",
    "        for k in list(inputs):\n",
    "            f = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                inputs[(n, im, i)] = self.to_tensor(f)\n",
    "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
    "                \n",
    "    \n",
    "\n",
    "                    \n",
    "    def check_depth(self):\n",
    "        line = self.filenames[0].split()\n",
    "        scene_name = line[0]\n",
    "        frame_index = int(line[1])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            scene_name,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        return os.path.isfile(velo_filename)\n",
    "    \n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "    #def get_color(self, folder, frame_index, side):\n",
    "        \n",
    "        color = self.loader(self.get_image_path(folder, frame_index, side))\n",
    "        \n",
    "        if do_flip:\n",
    "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return color\n",
    "    \n",
    "    def get_image_path(self, folder, frame_index, side):\n",
    "        f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\n",
    "        image_path = os.path.join(\n",
    "            self.data_path, folder, \"image_0{}/data\".format(self.side_map[side]), f_str)\n",
    "        return image_path\n",
    "\n",
    "    def get_depth(self, folder, frame_index, side):\n",
    "        calib_path = os.path.join(self.data_path, folder.split(\"/\")[0])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            folder,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\n",
    "        depth_gt = skimage.transform.resize(\n",
    "            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\n",
    "\n",
    "\n",
    "        return depth_gt\n",
    "    \n",
    "\n",
    "                    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        inputs = {}\n",
    "        \n",
    "        do_color_aug = self.is_train and random.random() > 0.5\n",
    "        do_flip = self.is_train and random.random() > 0.5\n",
    "        \n",
    "        line = self.filenames[index].split()\n",
    "        folder = line[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(line) == 3:\n",
    "            frame_index = int(line[1])\n",
    "            side = line[2]\n",
    "        else:\n",
    "            frame_index = 0\n",
    "            side = None\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            if i == \"s\":\n",
    "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\n",
    "                #inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side)\n",
    "            else:\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n",
    "                #inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side)\n",
    "                \n",
    "        for scale in range(self.num_scales):\n",
    "            K = self.K.copy()\n",
    "\n",
    "            K[0, :] *= self.width // (2 ** scale)\n",
    "            K[1, :] *= self.height // (2 ** scale)\n",
    "\n",
    "            inv_K = np.linalg.pinv(K)\n",
    "\n",
    "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
    "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
    "\n",
    "        if do_color_aug:\n",
    "            color_aug = transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        else:\n",
    "            color_aug =  (lambda x: x)\n",
    "            \n",
    "        self.preprocess(inputs,color_aug)\n",
    "        \n",
    "        for i in self.frame_idxs:\n",
    "            del inputs[(\"color\", i, -1)]\n",
    "            del inputs[(\"color_aug\", i, -1)]\n",
    "            \n",
    "        if self.load_depth:\n",
    "            depth_gt = self.get_depth(folder, frame_index, side)\n",
    "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
    "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
    "            \n",
    "        if \"s\" in self.frame_idxs:\n",
    "            stereo_T = np.eye(4, dtype=np.float32)\n",
    "            baseline_sign =  +1 #-1 if do_flip else 1\n",
    "            side_sign = -1 if side == \"l\" else 1\n",
    "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n",
    "\n",
    "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data according to files listed in eigen_full split \n",
    "\n",
    "dataset = datasets.KITTIRAWDataset \n",
    "data_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "fpath_train = \"/home/ubuntu/monodepth2/splits/eigen_zhou/train_files.txt\"\n",
    "f_train = open(fpath_train)\n",
    "train_filenames = f_train.readlines()\n",
    "fpath_val = \"/home/ubuntu/monodepth2/splits/eigen_zhou/val_files.txt\"\n",
    "f_val = open(fpath_val)\n",
    "val_filenames = f_val.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Size to rescale all input images to\n",
    "height = 192\n",
    "width = 640\n",
    "\n",
    "## Scales of the output image from each layer of the decoder\n",
    "## output scale = input scale/2**(n) for n in scales\n",
    "scales = np.arange(4)\n",
    "\n",
    "img_ext = '.jpg'\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "## List of frames that the dataloader must return \n",
    "## 0 - present frame\n",
    "frame_ids = [0,1,-1]\n",
    "\n",
    "\n",
    "\n",
    "weights_pretrained = True\n",
    "\n",
    "num_scales = len(scales)\n",
    "num_input_frames = len(frame_ids)\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "num_workers = 12\n",
    "\n",
    "train_dataset = dataset(data_path, train_filenames, height, width,frame_ids,num_scales,is_train=True, img_ext=img_ext)\n",
    "train_loader = DataLoader(train_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = dataset(data_path, val_filenames, height, width,frame_ids, 4, is_train=False, img_ext=img_ext)\n",
    "val_loader = DataLoader(val_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using resnet\n",
      "<class 'torchvision.models.resnet.BasicBlock'>\n"
     ]
    }
   ],
   "source": [
    "num_layer = 18\n",
    "\n",
    "params = []\n",
    "\n",
    "## Initialize the models\n",
    "\n",
    "encoder = networks.ResnetEncoder(num_layer, weights_pretrained)\n",
    "params += list(encoder.parameters())\n",
    "\n",
    "\n",
    "decoder = networks.DepthDecoder(encoder.num_ch_enc, scales)\n",
    "params += list(decoder.parameters())\n",
    "\n",
    "\n",
    "pose_enc = networks.ResnetEncoder(num_layer, weights_pretrained,num_input_images=2)\n",
    "params += list(pose_enc.parameters())\n",
    "pose_dec = networks.PoseDecoder(pose_enc.num_ch_enc,num_input_features=1,\n",
    "                    num_frames_to_predict_for=2)\n",
    "params += list(pose_dec.parameters())\n",
    "\n",
    "\n",
    "\n",
    "ssim = SSIM()\n",
    "ssim.to(device)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "pose_enc.to(device)\n",
    "pose_dec.to(device)\n",
    "\n",
    "optimizer = optim.Adam(params, learning_rate)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 15, 0.1)\n",
    "\n",
    "depth_metric_names = [\n",
    "            \"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "backproject_depth = {}\n",
    "project_3d = {}\n",
    "for scale in scales:\n",
    "    h = height // (2 ** scale)\n",
    "    w = width // (2 ** scale)\n",
    "\n",
    "    backproject_depth[scale] = BackprojectDepth(batch_size, h, w)\n",
    "    backproject_depth[scale].to(device)\n",
    "\n",
    "    project_3d[scale] = Project3D(batch_size, h, w)\n",
    "    project_3d[scale].to(device)\n",
    "#print(\"There are {:d} training items and {:d} validation items\\n\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images_pred(inputs, outputs):\n",
    "        \n",
    "   \n",
    "    source_scale = 0\n",
    "    \n",
    "    for scale in scales:\n",
    "        disp = outputs[(\"disp\", scale)]\n",
    "\n",
    "        disp = F.interpolate(disp, [height, width], mode=\"bilinear\",align_corners=False)\n",
    "        \n",
    "        depth = 1 / (0.01 + 9.99*disp)\n",
    " \n",
    "        outputs[(\"depth\", 0, scale)] = depth\n",
    "    \n",
    "        for frame_id in frame_ids[1:]:\n",
    "            T = outputs[(\"pred_pose\",0,frame_id)]\n",
    "\n",
    "            cam_points = backproject_depth[source_scale](depth, inputs[(\"inv_K\", source_scale)])\n",
    "            pix_coords = project_3d[source_scale](cam_points, inputs[(\"K\", source_scale)], T)\n",
    "\n",
    "            outputs[(\"color\",frame_id, scale)] = F.grid_sample(inputs[(\"color\",frame_id, source_scale)],\n",
    "                pix_coords,padding_mode=\"border\",align_corners=False)\n",
    "            \n",
    "            \n",
    "            outputs[(\"color_identity\",frame_id,scale)] = inputs[(\"color\",frame_id,source_scale)]\n",
    "\n",
    "    return outputs   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_poses(inputs,features):\n",
    "\n",
    "    outputs = {}\n",
    "    pose_features = {i:inputs[\"color_aug\", i, 0] for i in frame_ids}\n",
    "    for frame_id in frame_ids:\n",
    "        if frame_id < 0:\n",
    "            pose_inputs = [pose_features[frame_id],pose_features[0]]\n",
    "        else:\n",
    "            pose_inputs = [pose_features[0],pose_features[frame_id]]\n",
    "    \n",
    "        pose_inputs = [pose_enc(torch.cat(pose_inputs,1))]\n",
    "\n",
    "        axisangle,translation = pose_dec(pose_inputs)\n",
    "        \n",
    "        outputs[(\"axisangle\",0,frame_id)] = axisangle\n",
    "        outputs[(\"translation\",0,frame_id)] = translation\n",
    "        \n",
    "        outputs[(\"pred_pose\", 0, frame_id)] = transformation_from_parameters(\n",
    "                        axisangle[:, 0], translation[:, 0], invert=(frame_id < 0))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_reprojection_loss(pred, target):\n",
    "\n",
    "\n",
    "        l1_loss = (torch.abs(target - pred)).mean(1,True)\n",
    "        \n",
    "        ssim_loss = ssim(pred, target).mean(1, True)\n",
    "        reprojection_loss = 0.85*ssim_loss + 0.15 * l1_loss\n",
    "\n",
    "        return reprojection_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_loss(disp, img):\n",
    "\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x *= torch.exp(-grad_img_x)\n",
    "    grad_disp_y *= torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_losses(inputs, outputs):\n",
    "\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "        \n",
    "\n",
    "        for scale in scales:\n",
    "            loss = 0\n",
    "            reprojection_losses = []\n",
    "\n",
    "            source_scale = 0\n",
    "\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, source_scale)]\n",
    "\n",
    "            for frame_id in frame_ids[1:]:\n",
    "                pred = outputs[(\"color\", frame_id, scale)]\n",
    "                reprojection_losses.append(compute_reprojection_loss(pred, target))\n",
    "\n",
    "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
    "            \n",
    "            \n",
    "            #For automasking, find the pixels where the unwarped reprojection loss is less than warped\n",
    "            identity_reproj_loss = []\n",
    "            \n",
    "            for frame_id in frame_ids[1:]:\n",
    "                pred = inputs[(\"color\",frame_id,source_scale)]\n",
    "                identity_reproj_loss.append(compute_reprojection_loss(pred,target))\n",
    "                \n",
    "            identity_reproj_loss = torch.cat(identity_reproj_loss, 1)\n",
    "                \n",
    "                \n",
    "            identity_reproj_loss += torch.randn(identity_reproj_loss.shape).cuda() *1e-5\n",
    "            \n",
    "            \n",
    "            reprojection_losses = torch.cat((identity_reproj_loss, reprojection_losses), dim=1)\n",
    " \n",
    "        \n",
    "            to_optimise, idxs = torch.min(reprojection_losses, dim=1)\n",
    "            #to_optimise = reprojection_losses\n",
    "            \n",
    "            outputs[\"identity_selection/{}\".format(scale)] = (idxs > identity_reproj_loss.shape[1]-1).float()\n",
    "            \n",
    "            \n",
    "            loss = loss + to_optimise.mean()\n",
    "\n",
    "            norm_disp = disp / (disp.mean(2, True).mean(3, True)+ 1e-7)\n",
    "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
    "\n",
    "            loss =  loss + ((1e-3)*smooth_loss) / (2 ** scale)\n",
    "            total_loss = total_loss + loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "\n",
    "        total_loss = total_loss/num_scales\n",
    "        losses[\"loss\"] = total_loss\n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_depth_losses(inputs, outputs, losses):\n",
    "\n",
    "\n",
    "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
    "        \n",
    "        #Resize and clamp the values from 1e-3 to 80\n",
    "        depth_pred = torch.clamp(F.interpolate(\n",
    "            depth_pred, [375, 1242], mode=\"bilinear\", align_corners=False), min=1e-3, max=80)\n",
    "        depth_pred = depth_pred.detach()\n",
    "\n",
    "        depth_gt = inputs[\"depth_gt\"]\n",
    "        mask = depth_gt > 0\n",
    "\n",
    "        crop_mask = torch.zeros_like(mask)\n",
    "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * crop_mask\n",
    "\n",
    "        gt = depth_gt[mask]\n",
    "        pred = depth_pred[mask]\n",
    "        pred = pred*(torch.median(gt) / torch.median(pred))\n",
    "\n",
    "        pred = torch.clamp(pred, min=1e-3, max=80)\n",
    "        thresh = torch.max((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).float().mean()\n",
    "        a2 = (thresh < 1.25 ** 2).float().mean()\n",
    "        a3 = (thresh < 1.25 ** 3).float().mean()\n",
    "\n",
    "        rmse = torch.sqrt(((gt - pred)**2).mean())\n",
    "        rmse_log = torch.sqrt(((torch.log(gt) - torch.log(pred))**2).mean())\n",
    "        abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
    "        sq_rel = torch.mean(((gt - pred)**2 )/ gt)\n",
    "        \n",
    "        return np.array([abs_rel.item(),sq_rel.item(),rmse.item(),rmse_log.item(),a1.item(),a2.item(),a3.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader,val_loader,num_epochs):\n",
    "              \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "#         validation(encoder,decoder,val_loader)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            lr_scheduler.step()\n",
    "            total_loss = 0 \n",
    "            outer = tqdm(total=(len(train_loader)), desc='Training Epoch', position=0)\n",
    "\n",
    "            for batch_idx, inputs in enumerate(train_loader):\n",
    "\n",
    "                outer.update(1)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                outputs.update(predict_poses(inputs,features))\n",
    "                \n",
    "       \n",
    "                outputs = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "\n",
    "                losses = compute_losses(inputs, outputs)\n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                \n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                #lr_scheduler.step()\n",
    "\n",
    "                del inputs, outputs, features\n",
    "                if batch_idx%1000==0 and batch_idx>0:\n",
    "                    print('Batch No: ',batch_idx)\n",
    "                    #print('Loss: ',total_loss/(batch_idx+1))\n",
    "                    print('Loss: ',losses[\"loss\"].cpu().data)\n",
    "                del losses\n",
    "                    \n",
    "  \n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            print('Train Loss at Epoch_{}:'.format(epoch+1),total_loss)\n",
    "            validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,val_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_loss = 0\n",
    "        metrics = np.zeros((7))\n",
    "        for batch_idx, inputs in enumerate(val_loader):\n",
    "\n",
    "            before_op_time = time.time()\n",
    "\n",
    "            for key, ipt in inputs.items():\n",
    "                inputs[key] = ipt.to(device)\n",
    "\n",
    "            features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "            outputs = decoder(features)\n",
    "            \n",
    "            outputs.update(predict_poses(inputs,features))\n",
    "            outputs = generate_images_pred(inputs, outputs)\n",
    "            losses = compute_losses(inputs, outputs)\n",
    "            total_loss = total_loss + losses['loss'].item()\n",
    "         \n",
    "            metrics = metrics + compute_depth_losses(inputs, outputs, losses)\n",
    "            del inputs, outputs, losses,features\n",
    "         \n",
    "        total_loss = total_loss/len(val_loader)\n",
    "        print('Total Loss (Validation): ',total_loss)\n",
    "        print('Error Metrics: ',metrics/len(val_loader))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Training Epoch:  60%|██████    | 1002/1658 [20:09<13:16,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.1180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1658/1658 [32:56<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_1: 0.12019705796665967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/1658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.10759045511646115\n",
      "Error Metrics:  [0.15932202 1.07133012 6.07720853 0.25066966 0.74986157 0.91796807\n",
      " 0.97228466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  60%|██████    | 1001/1658 [20:20<12:36,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.0998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1658/1658 [33:30<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_2: 0.10423502622596893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/1658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.1015115427258222\n",
      "Error Metrics:  [0.16293155 1.21995237 5.53268363 0.23385562 0.77818487 0.93951901\n",
      " 0.97891202]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  60%|██████    | 1002/1658 [20:07<12:37,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.0856)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1658/1658 [32:46<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_3: 0.09662588981437453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/1658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.09634295280050972\n",
      "Error Metrics:  [0.15048612 1.0658405  5.06812091 0.2224218  0.81132144 0.94460461\n",
      " 0.97968045]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  60%|██████    | 1002/1658 [19:58<13:32,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.0960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1658/1658 [32:43<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_4: 0.09270336821369728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/1658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.09224443538519352\n",
      "Error Metrics:  [0.14292947 0.94684062 4.88535969 0.22444071 0.81155915 0.94711925\n",
      " 0.98026102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  60%|██████    | 1002/1658 [20:05<12:44,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.0961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1658/1658 [32:50<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_5: 0.08982515320253746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/1658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.08979287654485392\n",
      "Error Metrics:  [0.13913221 0.90247577 4.84808902 0.21742528 0.82697595 0.94629432\n",
      " 0.98170933]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  60%|██████    | 1002/1658 [20:00<11:53,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch No:  1000\n",
      "Loss:  tensor(0.0818)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1658/1658 [32:45<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch_6: 0.08804955835868711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/1658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.0868982670666731\n",
      "Error Metrics:  [0.13959333 0.97426409 4.89896654 0.22067987 0.83176763 0.94458453\n",
      " 0.98040986]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   8%|▊         | 133/1658 [02:53<30:47,  1.21s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "total_loss = 0\n",
    "train_loader = val_loader\n",
    "for batch_idx, inputs in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "                \n",
    "                outputs.update(predict_poses(inputs,features))\n",
    "               \n",
    "                o1 = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "                \n",
    "                losses = compute_losses(inputs, o1)\n",
    "                \n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                optimizer.zero_grad()\n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                i = i+1\n",
    "                \n",
    "                lr_scheduler.step()\n",
    "                \n",
    "                if i>2:\n",
    "                    break\n",
    "                   \n",
    "                del inputs, outputs, losses,features,o1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[('disp', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/ubuntu/monodepth2/assets/Home_Hero5.jpg\"\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    features = encoder(input_image_pytorch.to(device))\n",
    "    outputs = decoder(features)\n",
    "\n",
    "disp = outputs[(\"disp\", 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "# Saving colormapped depth image\n",
    "disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input\", fontsize=22)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "plt.title(\"Disparity prediction\", fontsize=22)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_dataset[0][('color_aug', 's', 0)]\n",
    "b = val_dataset[0][('color', 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ss(nn.Module):\n",
    "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ss, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.refl(x)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "\n",
    "ftest = \"/home/ubuntu/monodepth2/splits/eigen/test_files.txt\"\n",
    "test_filenames = open(ftest).readlines()\n",
    "\n",
    "\n",
    "for f in test_filenames:\n",
    "    path = f.split(\" \")\n",
    "    \n",
    "    \n",
    "    if path[2]=='l\\n':\n",
    "        image_path = os.path.join(test_path,path[0],\"image_03/data\",path[1])\n",
    "    else:\n",
    "        image_path = os.path.join(test_path,path[0],\"image_04/data\",path[1])\n",
    "    print(image_path)\n",
    "    input_image = pil.open(image_path+\".jpg\").convert('RGB')\n",
    "    original_width, original_height = input_image.size\n",
    "    input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        features = encoder(input_image_pytorch.to(device))\n",
    "        outputs = decoder(features)\n",
    "\n",
    "    disp = outputs[(\"disp\", 0)]\n",
    "    disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    # Saving colormapped depth image\n",
    "    disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "    vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.imshow(input_image)\n",
    "    plt.title(\"Input\", fontsize=22)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "    plt.title(\"Disparity prediction\", fontsize=22)\n",
    "    plt.axis('off');\n",
    "    \n",
    "    plt.savefig('output_imgs_10epochs/'+path[1]+'.png',dpi = 100)\n",
    "    \n",
    "    plt.close('all')\n",
    "    del outputs,features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/ubuntu/monodepth2/assets/test_image.jpg\"\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder,'Depthencoder_OG_2.pt')\n",
    "torch.save(decoder,'Depthdecoder_OG_2.pt')\n",
    "torch.save(pose_enc,'Poseencoder_OG_2.pt')\n",
    "torch.save(pose_dec,'Posedecoder_OG_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('encoder10e.pt')\n",
    "decoder = torch.load('decoder10e.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][('color',0,2)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
