{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monocular attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  \n",
    "\n",
    "import PIL.Image as pil\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "## Some helper functions are used from the original implementation\n",
    "from utils import *\n",
    "from kitti_utils import *\n",
    "from depthDecoder import *\n",
    "from pose_decoder import *\n",
    "from layers import *\n",
    "\n",
    "\n",
    "import datasets\n",
    "import networks\n",
    "from IPython import embed\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data according to files listed in eigen_full split \n",
    "\n",
    "dataset = datasets.KITTIRAWDataset \n",
    "data_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "fpath_train = \"/home/ubuntu/monodepth2/splits/eigen_zhou/train_files.txt\"\n",
    "f_train = open(fpath_train)\n",
    "train_filenames = f_train.readlines()\n",
    "fpath_val = \"/home/ubuntu/monodepth2/splits/eigen_zhou/val_files.txt\"\n",
    "f_val = open(fpath_val)\n",
    "val_filenames = f_val.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Size to rescale all input images to\n",
    "height = 192\n",
    "width = 640\n",
    "\n",
    "## Scales of the output image from each layer of the decoder\n",
    "## output scale = input scale/2**(n) for n in scales\n",
    "scales = np.arange(4)\n",
    "\n",
    "img_ext = '.jpg'\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "## List of frames that the dataloader must return \n",
    "## 0 - present frame\n",
    "frame_ids = [0,1,-1]\n",
    "\n",
    "\n",
    "\n",
    "weights_pretrained = True\n",
    "\n",
    "num_scales = len(scales)\n",
    "num_input_frames = len(frame_ids)\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 6\n",
    "\n",
    "train_dataset = dataset(data_path, train_filenames, height, width,frame_ids,num_scales,is_train=True, img_ext=img_ext)\n",
    "train_loader = DataLoader(train_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = dataset(data_path, val_filenames, height, width,frame_ids, 4, is_train=False, img_ext=img_ext)\n",
    "val_loader = DataLoader(val_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthEncoderModule(nn.Module):\n",
    "    def __init__(self, layers, num_input_images = 1):\n",
    "        super(DepthEncoderModule, self).__init__()\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "        pretrained = True\n",
    "        self.encoder = models.resnet18(pretrained)\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "    \n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEncoderModule(nn.Module):\n",
    "    def __init__(self, layers, num_input_images = 1):\n",
    "        super(PoseEncoderModule, self).__init__()\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "        pretrained = True\n",
    "        self.encoder = models.resnet18(pretrained)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        self.features.append(self.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "    \n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 18\n",
    "\n",
    "params = []\n",
    "\n",
    "## Initialize the models\n",
    "\n",
    "# encoder = networks.ResnetEncoder(num_layer, weights_pretrained)\n",
    "encoder = DepthEncoderModule(layers = 18)\n",
    "params += list(encoder.parameters())\n",
    "\n",
    "\n",
    "# decoder = networks.DepthDecoder(encoder.num_ch_enc, scales)\n",
    "decoder = Decoder(encoder.num_ch_enc, scales)\n",
    "params += list(decoder.parameters())\n",
    "\n",
    "num_input_images = 2\n",
    "# pose_enc = networks.ResnetEncoder(num_layer, weights_pretrained,num_input_images=2)\n",
    "pose_enc = PoseEncoderModule(layers = 18, num_input_images=num_input_images)\n",
    "\n",
    "pose_enc.conv1 = nn.Conv2d(num_input_images*3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "pose_enc.maxpool =  nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "pose_enc.bn1 = nn.BatchNorm2d(64)\n",
    "pose_enc.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(18)])\n",
    "new_weights = torch.cat([loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
    "pose_enc.conv1.weight = torch.nn.Parameter(new_weights)\n",
    "\n",
    "params += list(pose_enc.parameters())\n",
    "\n",
    "# pose_dec = networks.PoseDecoder(pose_enc.num_ch_enc,num_input_features=1,\n",
    "#                     num_frames_to_predict_for=2)\n",
    "pose_dec = PoseDecoder(pose_enc.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "params += list(pose_dec.parameters())\n",
    "\n",
    "\n",
    "\n",
    "ssim = SSIM()\n",
    "ssim.to(device)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "pose_enc.to(device)\n",
    "pose_dec.to(device)\n",
    "\n",
    "optimizer = optim.Adam(params, learning_rate)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 15, 0.1)\n",
    "\n",
    "depth_metric_names = [\n",
    "            \"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "backproject_depth = {}\n",
    "project_3d = {}\n",
    "for scale in scales:\n",
    "    h = height // (2 ** scale)\n",
    "    w = width // (2 ** scale)\n",
    "\n",
    "    backproject_depth[scale] = BackprojectDepth(batch_size, h, w)\n",
    "    backproject_depth[scale].to(device)\n",
    "\n",
    "    project_3d[scale] = Project3D(batch_size, h, w)\n",
    "    project_3d[scale].to(device)\n",
    "#print(\"There are {:d} training items and {:d} validation items\\n\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images_pred(inputs, outputs):\n",
    "        \n",
    "   \n",
    "    source_scale = 0\n",
    "    \n",
    "    for scale in scales:\n",
    "        disp = outputs[(\"disp\", scale)]\n",
    "\n",
    "        disp = F.interpolate(disp, [height, width], mode=\"bilinear\",align_corners=False)\n",
    "        \n",
    "        depth = 1 / (0.01 + 9.99*disp)\n",
    " \n",
    "        outputs[(\"depth\", 0, scale)] = depth\n",
    "    \n",
    "        for frame_id in frame_ids[1:]:\n",
    "            T = outputs[(\"pred_pose\",0,frame_id)]\n",
    "\n",
    "            cam_points = backproject_depth[source_scale](depth, inputs[(\"inv_K\", source_scale)])\n",
    "            pix_coords = project_3d[source_scale](cam_points, inputs[(\"K\", source_scale)], T)\n",
    "\n",
    "            outputs[(\"color\",frame_id, scale)] = F.grid_sample(inputs[(\"color\",frame_id, source_scale)],\n",
    "                pix_coords,padding_mode=\"border\",align_corners=False)\n",
    "            \n",
    "            \n",
    "            outputs[(\"color_identity\",frame_id,scale)] = inputs[(\"color\",frame_id,source_scale)]\n",
    "\n",
    "    return outputs   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_poses(inputs,features):\n",
    "\n",
    "    outputs = {}\n",
    "    pose_features = {i:inputs[\"color_aug\", i, 0] for i in frame_ids}\n",
    "    for frame_id in frame_ids:\n",
    "        if frame_id < 0:\n",
    "            pose_inputs = [pose_features[frame_id],pose_features[0]]\n",
    "        else:\n",
    "            pose_inputs = [pose_features[0],pose_features[frame_id]]\n",
    "    \n",
    "        pose_inputs = [pose_enc(torch.cat(pose_inputs,1))]\n",
    "\n",
    "        axisangle,translation = pose_dec(pose_inputs)\n",
    "        \n",
    "        outputs[(\"axisangle\",0,frame_id)] = axisangle\n",
    "        outputs[(\"translation\",0,frame_id)] = translation\n",
    "        \n",
    "        outputs[(\"pred_pose\", 0, frame_id)] = transformation_from_parameters(\n",
    "                        axisangle[:, 0], translation[:, 0], invert=(frame_id < 0))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import exp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)*2/float(2*sigma*2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim_new(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_reprojection_loss(pred, target):\n",
    "    l1_loss = (torch.abs(target - pred)).mean(1,True)\n",
    "    ssim_loss = torch.clamp((1 - ssim_new(pred, target,1)) * 0.5, 0, 1) #torch.clamp(ssim_new(pred, target)).mean(1, True)\n",
    "    #alpha = 0.95\n",
    "    #reprojection_loss = (alpha)*ssim_loss + (1-alpha) * l1_loss\n",
    "    reprojection_loss = 1e-2*ssim_loss + 0.15* l1_loss + 0.85*ssim(pred, target).mean(1, True)\n",
    "    return reprojection_loss\n",
    "\n",
    "\n",
    "#         l1_loss = (torch.abs(target - pred)).mean(1,True)\n",
    "        \n",
    "#         ssim_loss = ssim(pred, target).mean(1, True)\n",
    "#         reprojection_loss = 0.85*ssim_loss + 0.15 * l1_loss\n",
    "\n",
    "#         return reprojection_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_loss(disp, img):\n",
    "\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x *= torch.exp(-grad_img_x)\n",
    "    grad_disp_y *= torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_losses(inputs, outputs):\n",
    "\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "        \n",
    "\n",
    "        for scale in scales:\n",
    "            loss = 0\n",
    "            reprojection_losses = []\n",
    "\n",
    "            source_scale = 0\n",
    "\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, source_scale)]\n",
    "\n",
    "            for frame_id in frame_ids[1:]:\n",
    "                pred = outputs[(\"color\", frame_id, scale)]\n",
    "                reprojection_losses.append(compute_reprojection_loss(pred, target))\n",
    "\n",
    "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
    "            \n",
    "            \n",
    "            #For automasking, find the pixels where the unwarped reprojection loss is less than warped\n",
    "            identity_reproj_loss = []\n",
    "            \n",
    "            for frame_id in frame_ids[1:]:\n",
    "                pred = inputs[(\"color\",frame_id,source_scale)]\n",
    "                identity_reproj_loss.append(compute_reprojection_loss(pred,target))\n",
    "                \n",
    "            identity_reproj_loss = torch.cat(identity_reproj_loss, 1)\n",
    "                \n",
    "                \n",
    "            identity_reproj_loss += torch.randn(identity_reproj_loss.shape).cuda() *1e-5\n",
    "            \n",
    "            \n",
    "            reprojection_losses = torch.cat((identity_reproj_loss, reprojection_losses), dim=1)\n",
    " \n",
    "        \n",
    "            to_optimise, idxs = torch.min(reprojection_losses, dim=1)\n",
    "            #to_optimise = reprojection_losses\n",
    "            \n",
    "            outputs[\"identity_selection/{}\".format(scale)] = (idxs > identity_reproj_loss.shape[1]-1).float()\n",
    "            \n",
    "            \n",
    "            loss = loss + to_optimise.mean()\n",
    "\n",
    "            norm_disp = disp / (disp.mean(2, True).mean(3, True)+ 1e-7)\n",
    "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
    "\n",
    "#             loss =  loss + ((1e-5)*smooth_loss) / (2 ** scale)\n",
    "            loss =  loss + (1e-3)*(smooth_loss) / (2 ** scale)\n",
    "            total_loss = total_loss + loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "\n",
    "        total_loss = total_loss/num_scales\n",
    "        losses[\"loss\"] = total_loss\n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_depth_losses(inputs, outputs, losses):\n",
    "\n",
    "\n",
    "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
    "        \n",
    "        #Resize and clamp the values from 1e-3 to 80\n",
    "        depth_pred = torch.clamp(F.interpolate(\n",
    "            depth_pred, [375, 1242], mode=\"bilinear\", align_corners=False), min=1e-3, max=80)\n",
    "        depth_pred = depth_pred.detach()\n",
    "\n",
    "        depth_gt = inputs[\"depth_gt\"]\n",
    "        mask = depth_gt > 0\n",
    "\n",
    "        crop_mask = torch.zeros_like(mask)\n",
    "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * crop_mask\n",
    "\n",
    "        gt = depth_gt[mask]\n",
    "        pred = depth_pred[mask]\n",
    "        pred = pred*(torch.median(gt) / torch.median(pred))\n",
    "\n",
    "        pred = torch.clamp(pred, min=1e-3, max=80)\n",
    "        thresh = torch.max((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).float().mean()\n",
    "        a2 = (thresh < 1.25 ** 2).float().mean()\n",
    "        a3 = (thresh < 1.25 ** 3).float().mean()\n",
    "\n",
    "        rmse = torch.sqrt(((gt - pred)**2).mean())\n",
    "        rmse_log = torch.sqrt(((torch.log(gt) - torch.log(pred))**2).mean())\n",
    "        abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
    "        sq_rel = torch.mean(((gt - pred)**2 )/ gt)\n",
    "        \n",
    "        return np.array([abs_rel.item(),sq_rel.item(),rmse.item(),rmse_log.item(),a1.item(),a2.item(),a3.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader,val_loader,num_epochs):\n",
    "              \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for epoch in range(0, num_epochs):\n",
    "#             lr_scheduler.step()\n",
    "            total_loss = 0 \n",
    "            outer = tqdm(total=(len(train_loader)), desc='Training Epoch', position=0)\n",
    "            print('epoch')\n",
    "\n",
    "            for batch_idx, inputs in enumerate(train_loader):\n",
    "\n",
    "                outer.update(1)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "                                \n",
    "                outputs.update(predict_poses(inputs,features))\n",
    "                       \n",
    "                outputs = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "                losses = compute_losses(inputs, outputs)\n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                \n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                del inputs, outputs, features\n",
    "                if batch_idx%1000==0 and batch_idx>0:\n",
    "                    print('Batch No: ',batch_idx)\n",
    "                    #print('Loss: ',total_loss/(batch_idx+1))\n",
    "                    print('Loss: ',losses[\"loss\"].cpu().data)\n",
    "                del losses\n",
    "                    \n",
    "  \n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            print('Train Loss at Epoch_{}:'.format(epoch+1),total_loss)\n",
    "            validation(encoder,decoder,val_loader)\n",
    "            run_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,val_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_loss = 0\n",
    "        metrics = np.zeros((7))\n",
    "        for batch_idx, inputs in enumerate(val_loader):\n",
    "\n",
    "            before_op_time = time.time()\n",
    "\n",
    "            for key, ipt in inputs.items():\n",
    "                inputs[key] = ipt.to(device)\n",
    "\n",
    "            features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "            outputs = decoder(features)\n",
    "            \n",
    "            outputs.update(predict_poses(inputs,features))\n",
    "            outputs = generate_images_pred(inputs, outputs)\n",
    "            losses = compute_losses(inputs, outputs)\n",
    "            total_loss = total_loss + losses['loss'].item()\n",
    "         \n",
    "            metrics = metrics + compute_depth_losses(inputs, outputs, losses)\n",
    "            del inputs, outputs, losses,features\n",
    "         \n",
    "        total_loss = total_loss/len(val_loader)\n",
    "        print('Total Loss (Validation): ',total_loss)\n",
    "        print('Error Metrics: ',metrics/len(val_loader))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/4976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 2/4976 [00:04<3:48:12,  2.75s/it]"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "train(encoder,decoder,train_loader,val_loader,num_epochs)\n",
    "#     run_images(epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "total_loss = 0\n",
    "train_loader = val_loader\n",
    "for batch_idx, inputs in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "                \n",
    "                outputs.update(predict_poses(inputs,features))\n",
    "               \n",
    "                o1 = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "                \n",
    "                losses = compute_losses(inputs, o1)\n",
    "                \n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                optimizer.zero_grad()\n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                i = i+1\n",
    "                \n",
    "                lr_scheduler.step()\n",
    "                \n",
    "                if i>2:\n",
    "                    break\n",
    "                   \n",
    "                del inputs, outputs, losses,features,o1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[('disp', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/ubuntu/monodepth2/assets/Home_Hero5.jpg\"\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'CompleteSmoothLoss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(epoch_num):\n",
    "    torch.save(encoder,'MonocularOutput/Models/Resnet18/'+folder_name+'/DepthEncoder_'+str(epoch_num)+'.pt')\n",
    "    torch.save(decoder,'MonocularOutput/Models/Resnet18/'+folder_name+'/DepthDecoder_'+str(epoch_num)+'.pt')\n",
    "    torch.save(pose_enc,'MonocularOutput/Models/Resnet18/'+folder_name+'/PoseEncoder_'+str(epoch_num)+'.pt')\n",
    "    torch.save(pose_dec,'MonocularOutput/Models/Resnet18/'+folder_name+'/PoseDecoder_'+str(epoch_num)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_images(epoch_num):\n",
    "    directory = 'Iteration '+str(epoch_num)+'/'\n",
    "    parent_dir = 'MonocularOutput/Resnet18/'+folder_name+'/'\n",
    "    path = os.path.join(parent_dir, directory) \n",
    "    os.mkdir(path) \n",
    "    image_output_path = parent_dir+directory\n",
    "    for i in range(1, 9):\n",
    "        image_name = 'Test'+str(i)+'.jpg'\n",
    "        image_path = '/home/ubuntu/monodepth2/assets/'+image_name\n",
    "        input_image = pil.open(image_path).convert('RGB')\n",
    "        original_width, original_height = input_image.size\n",
    "        input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "        input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            features = encoder(input_image_pytorch.to(device))\n",
    "            outputs = decoder(features)\n",
    "\n",
    "        disp = outputs[(\"disp\", 0)]\n",
    "        disp_resized = torch.nn.functional.interpolate(disp,(original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        # Saving colormapped depth image\n",
    "        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "        vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(input_image)\n",
    "        plt.title(\"Input\", fontsize=22)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "        plt.title(\"Disparity prediction\", fontsize=22)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(image_output_path+image_name,dpi = 100)\n",
    "    saveModel(epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_dataset[0][('color_aug', 's', 0)]\n",
    "b = val_dataset[0][('color', 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ss(nn.Module):\n",
    "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ss, self).__init__()\n",
    "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
    "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
    "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
    "\n",
    "        self.refl = nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.C1 = 0.01 ** 2\n",
    "        self.C2 = 0.03 ** 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.refl(x)\n",
    "        y = self.refl(y)\n",
    "\n",
    "        mu_x = self.mu_x_pool(x)\n",
    "        mu_y = self.mu_y_pool(y)\n",
    "\n",
    "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
    "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
    "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
    "\n",
    "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
    "\n",
    "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "\n",
    "ftest = \"/home/ubuntu/monodepth2/splits/eigen/test_files.txt\"\n",
    "test_filenames = open(ftest).readlines()\n",
    "\n",
    "\n",
    "for f in test_filenames:\n",
    "    path = f.split(\" \")\n",
    "    \n",
    "    \n",
    "    if path[2]=='l\\n':\n",
    "        image_path = os.path.join(test_path,path[0],\"image_03/data\",path[1])\n",
    "    else:\n",
    "        image_path = os.path.join(test_path,path[0],\"image_04/data\",path[1])\n",
    "    print(image_path)\n",
    "    input_image = pil.open(image_path+\".jpg\").convert('RGB')\n",
    "    original_width, original_height = input_image.size\n",
    "    input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        features = encoder(input_image_pytorch.to(device))\n",
    "        outputs = decoder(features)\n",
    "\n",
    "    disp = outputs[(\"disp\", 0)]\n",
    "    disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    # Saving colormapped depth image\n",
    "    disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "    vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.imshow(input_image)\n",
    "    plt.title(\"Input\", fontsize=22)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "    plt.title(\"Disparity prediction\", fontsize=22)\n",
    "    plt.axis('off');\n",
    "    \n",
    "    plt.savefig('output_imgs_10epochs/'+path[1]+'.png',dpi = 100)\n",
    "    \n",
    "    plt.close('all')\n",
    "    del outputs,features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/ubuntu/monodepth2/assets/test_image.jpg\"\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('encoder10e.pt')\n",
    "decoder = torch.load('decoder10e.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][('color',0,2)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name ='IMG_20200428_191113.jpg'\n",
    "image_path = '/home/ubuntu/monodepth2/assets/'+image_name\n",
    "input_image = pil.open(image_path).convert('RGB')\n",
    "original_width, original_height = input_image.size\n",
    "input_image_resized = input_image.resize((width, height), pil.LANCZOS)\n",
    "input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    features = encoder(input_image_pytorch.to(device))\n",
    "    outputs = decoder(features)\n",
    "\n",
    "disp = outputs[(\"disp\", 0)]\n",
    "disp_resized = torch.nn.functional.interpolate(disp,(original_height, original_width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "# Saving colormapped depth image\n",
    "disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image)\n",
    "plt.title(\"Input\", fontsize=22)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "plt.title(\"Disparity prediction\", fontsize=22)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_images(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_images(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
