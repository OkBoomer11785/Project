{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT RUN THIS CELL..WORK IN PROGRESS\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  \n",
    "\n",
    "import PIL.Image as pil\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from collections import Counter\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import json\n",
    "\n",
    "from utils import *\n",
    "from kitti_utils import *\n",
    "from layers import *\n",
    "\n",
    "import datasets\n",
    "import networks\n",
    "from IPython import embed\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL..WORK IN PROGRESS\n",
    "\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL..WORK IN PROGRESS\n",
    "\n",
    "class MainDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,data_path,filenames,height,width,frame_idxs,num_scales,is_train=False,img_ext='.jpg'):\n",
    "        super(MainDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.filenames = filenames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_scales = num_scales\n",
    "        self.frame_idxs = frame_idxs\n",
    "        self.is_train = is_train\n",
    "        self.img_ext = img_ext\n",
    "        \n",
    "        self.interp = Image.ANTIALIAS\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            self.brightness = (0.8, 1.2)\n",
    "            self.contrast = (0.8, 1.2)\n",
    "            self.saturation = (0.8, 1.2)\n",
    "            self.hue = (-0.1, 0.1)\n",
    "            transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        except TypeError:\n",
    "            self.brightness = 0.2\n",
    "            self.contrast = 0.2\n",
    "            self.saturation = 0.2\n",
    "            self.hue = 0.1\n",
    "        \n",
    "        \n",
    "        self.resize = {}\n",
    "        for i in range(self.num_scales):\n",
    "            s = 2 ** i\n",
    "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),interpolation=self.interp)\n",
    "            \n",
    "        self.load_depth = self.check_depth()\n",
    "        \n",
    "        self.K = np.array([[0.58, 0, 0.5, 0],\n",
    "                           [0, 1.92, 0.5, 0],\n",
    "                           [0, 0, 1, 0],\n",
    "                           [0, 0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "        self.full_res_shape = (1242, 375)\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def preprocess(self, inputs, color_aug):\n",
    "        \n",
    "        for k in list(inputs):\n",
    "            frame = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                for i in range(self.num_scales):\n",
    "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
    "\n",
    "        for k in list(inputs):\n",
    "            f = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                inputs[(n, im, i)] = self.to_tensor(f)\n",
    "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
    "\n",
    "                    \n",
    "    def check_depth(self):\n",
    "        line = self.filenames[0].split()\n",
    "        scene_name = line[0]\n",
    "        frame_index = int(line[1])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            scene_name,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        return os.path.isfile(velo_filename)\n",
    "    \n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "        \n",
    "        color = self.loader(self.get_image_path(folder, frame_index, side))\n",
    "        if do_flip:\n",
    "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return color\n",
    "    \n",
    "    def get_image_path(self, folder, frame_index, side):\n",
    "        f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\n",
    "        image_path = os.path.join(\n",
    "            self.data_path, folder, \"image_0{}/data\".format(self.side_map[side]), f_str)\n",
    "        return image_path\n",
    "\n",
    "    def get_depth(self, folder, frame_index, side):\n",
    "        calib_path = os.path.join(self.data_path, folder.split(\"/\")[0])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            folder,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\n",
    "        depth_gt = skimage.transform.resize(\n",
    "            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\n",
    "\n",
    "\n",
    "        return depth_gt\n",
    "                    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        inputs = {}\n",
    "        \n",
    "        do_color_aug = self.is_train and random.random() > 0.5\n",
    "        do_flip = self.is_train and random.random() > 0.5\n",
    "        \n",
    "        line = self.filenames[index].split()\n",
    "        folder = line[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(line) == 3:\n",
    "            frame_index = int(line[1])\n",
    "            side = line[2]\n",
    "        else:\n",
    "            frame_index = 0\n",
    "            side = None\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            if i == \"s\":\n",
    "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\n",
    "            else:\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n",
    "                \n",
    "        for scale in range(self.num_scales):\n",
    "            K = self.K.copy()\n",
    "\n",
    "            K[0, :] *= self.width // (2 ** scale)\n",
    "            K[1, :] *= self.height // (2 ** scale)\n",
    "\n",
    "            inv_K = np.linalg.pinv(K)\n",
    "\n",
    "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
    "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
    "\n",
    "        if do_color_aug:\n",
    "            color_aug = transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        else:\n",
    "            color_aug = (lambda x: x)\n",
    "            \n",
    "        self.preprocess(inputs,color_aug)\n",
    "        \n",
    "        for i in self.frame_idxs:\n",
    "            del inputs[(\"color\", i, -1)]\n",
    "            #del inputs[(\"color_aug\", i, -1)]\n",
    "            \n",
    "        if self.load_depth:\n",
    "            depth_gt = self.get_depth(folder, frame_index, side)\n",
    "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
    "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
    "            \n",
    "        if \"s\" in self.frame_idxs:\n",
    "            stereo_T = np.eye(4, dtype=np.float32)\n",
    "            baseline_sign =  +1 #-1 if do_flip else 1\n",
    "            side_sign = -1 if side == \"l\" else 1\n",
    "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n",
    "\n",
    "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_velodyne_points(filename):\n",
    "    \"\"\"Load 3D point cloud from KITTI file format\n",
    "    (adapted from https://github.com/hunse/kitti)\n",
    "    \"\"\"\n",
    "    points = np.fromfile(filename, dtype=np.float32).reshape(-1, 4)\n",
    "    points[:, 3] = 1.0  # homogeneous\n",
    "    return points\n",
    "\n",
    "\n",
    "def read_calib_file(path):\n",
    "    \"\"\"Read KITTI calibration file\n",
    "    (from https://github.com/hunse/kitti)\n",
    "    \"\"\"\n",
    "    float_chars = set(\"0123456789.e+- \")\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split(':', 1)\n",
    "            value = value.strip()\n",
    "            data[key] = value\n",
    "            if float_chars.issuperset(value):\n",
    "                # try to cast to float array\n",
    "                try:\n",
    "                    data[key] = np.array(list(map(float, value.split(' '))))\n",
    "                except ValueError:\n",
    "                    # casting error: data[key] already eq. value, so pass\n",
    "                    pass\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def sub2ind(matrixSize, rowSub, colSub):\n",
    "    \"\"\"Convert row, col matrix subscripts to linear indices\n",
    "    \"\"\"\n",
    "    m, n = matrixSize\n",
    "    return rowSub * (n-1) + colSub - 1\n",
    "\n",
    "\n",
    "def generate_depth_map(calib_dir, velo_filename, cam=2, vel_depth=False):\n",
    "    \"\"\"Generate a depth map from velodyne data\n",
    "    \"\"\"\n",
    "    # load calibration files\n",
    "    cam2cam = read_calib_file(os.path.join(calib_dir, 'calib_cam_to_cam.txt'))\n",
    "    velo2cam = read_calib_file(os.path.join(calib_dir, 'calib_velo_to_cam.txt'))\n",
    "    velo2cam = np.hstack((velo2cam['R'].reshape(3, 3), velo2cam['T'][..., np.newaxis]))\n",
    "    velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n",
    "\n",
    "    # get image shape\n",
    "    im_shape = cam2cam[\"S_rect_02\"][::-1].astype(np.int32)\n",
    "\n",
    "    # compute projection matrix velodyne->image plane\n",
    "    R_cam2rect = np.eye(4)\n",
    "    R_cam2rect[:3, :3] = cam2cam['R_rect_00'].reshape(3, 3)\n",
    "    P_rect = cam2cam['P_rect_0'+str(cam)].reshape(3, 4)\n",
    "    P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n",
    "\n",
    "    # load velodyne points and remove all behind image plane (approximation)\n",
    "    # each row of the velodyne data is forward, left, up, reflectance\n",
    "    velo = load_velodyne_points(velo_filename)\n",
    "    velo = velo[velo[:, 0] >= 0, :]\n",
    "\n",
    "    # project the points to the camera\n",
    "    velo_pts_im = np.dot(P_velo2im, velo.T).T\n",
    "    velo_pts_im[:, :2] = velo_pts_im[:, :2] / velo_pts_im[:, 2][..., np.newaxis]\n",
    "\n",
    "    if vel_depth:\n",
    "        velo_pts_im[:, 2] = velo[:, 0]\n",
    "\n",
    "    # check if in bounds\n",
    "    # use minus 1 to get the exact same value as KITTI matlab code\n",
    "    velo_pts_im[:, 0] = np.round(velo_pts_im[:, 0]) - 1\n",
    "    velo_pts_im[:, 1] = np.round(velo_pts_im[:, 1]) - 1\n",
    "    val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n",
    "    val_inds = val_inds & (velo_pts_im[:, 0] < im_shape[1]) & (velo_pts_im[:, 1] < im_shape[0])\n",
    "    velo_pts_im = velo_pts_im[val_inds, :]\n",
    "\n",
    "    # project to image\n",
    "    depth = np.zeros((im_shape[:2]))\n",
    "    depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n",
    "\n",
    "    # find the duplicate points and choose the closest depth\n",
    "    inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n",
    "    dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n",
    "    for dd in dupe_inds:\n",
    "        pts = np.where(inds == dd)[0]\n",
    "        x_loc = int(velo_pts_im[pts[0], 0])\n",
    "        y_loc = int(velo_pts_im[pts[0], 1])\n",
    "        depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n",
    "    depth[depth < 0] = 0\n",
    "\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset Parameters\n",
    "data_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "height = 192\n",
    "width = 640\n",
    "\n",
    "num_scales = 4\n",
    "img_ext = '.jpg'\n",
    "\n",
    "#Loader Parameters\n",
    "batch_size = 12\n",
    "num_workers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_train = \"/home/ubuntu/monodepth2/splits/eigen_zhou/train_files.txt\"\n",
    "f_train = open(fpath_train)\n",
    "train_filenames = f_train.readlines()\n",
    "#train_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_val = \"/home/ubuntu/monodepth2/splits/eigen_zhou/val_files.txt\"\n",
    "f_val = open(fpath_val)\n",
    "val_filenames = f_val.readlines()\n",
    "#val_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_dict = {\"kitti\": datasets.KITTIRAWDataset}\n",
    "datasets_dict = {\"kitti\": MainDataset}\n",
    "dataset = datasets_dict[\"kitti\"]\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSIM(\n",
       "  (mu_x_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (mu_y_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (sig_x_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (sig_y_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (sig_xy_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (refl): ReflectionPad2d((1, 1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height = 192\n",
    "width = 640\n",
    "scales = np.arange(4)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "frame_ids = [0,'s']\n",
    "weights_init = 'pretrained'\n",
    "num_scales = len(scales)\n",
    "num_input_frames = len(frame_ids)\n",
    "num_layers = 18\n",
    "learning_rate = 0.001\n",
    "ssim = SSIM()\n",
    "ssim.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = dataset(data_path, train_filenames, height, width,frame_ids,num_scales,is_train=True, img_ext=img_ext)\n",
    "train_loader = DataLoader(train_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = dataset(data_path, val_filenames, height, width,frame_ids, 4, is_train=False, img_ext=img_ext)\n",
    "val_loader = DataLoader(val_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "encoder = networks.ResnetEncoder(num_layers, weights_init == \"pretrained\")\n",
    "params += list(encoder.parameters())\n",
    "\n",
    "decoder = networks.DepthDecoder(encoder.num_ch_enc, scales)\n",
    "params += list(decoder.parameters())\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "optimizer = optim.Adam(params, learning_rate)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 15, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45200 training items and 1776 validation items\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backproject_depth = {}\n",
    "project_3d = {}\n",
    "for scale in scales:\n",
    "    h = height // (2 ** scale)\n",
    "    w = width // (2 ** scale)\n",
    "\n",
    "    backproject_depth[scale] = BackprojectDepth(batch_size, h, w)\n",
    "    backproject_depth[scale].to(device)\n",
    "\n",
    "    project_3d[scale] = Project3D(batch_size, h, w)\n",
    "    project_3d[scale].to(device)\n",
    "print(\"There are {:d} training items and {:d} validation items\\n\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_pred(inputs, outputs):\n",
    "        \n",
    "    frame_id = 's'\n",
    "    for scale in scales:\n",
    "        disp = outputs[(\"disp\", scale)]\n",
    "\n",
    "        disp = F.interpolate(disp, [height, width], mode=\"bilinear\", align_corners=True)\n",
    "        source_scale = 0\n",
    "\n",
    "        _, depth = disp_to_depth(disp, 0.1, 100)\n",
    "\n",
    "        outputs[(\"depth\", 0, scale)] = depth\n",
    "\n",
    "        T = inputs[\"stereo_T\"]\n",
    "\n",
    "        cam_points = backproject_depth[source_scale](depth, inputs[(\"inv_K\", source_scale)])\n",
    "        pix_coords = project_3d[source_scale](cam_points, inputs[(\"K\", source_scale)], T)\n",
    "        outputs[(\"coords\", frame_id, scale)] = pix_coords\n",
    "        outputs[(\"color\", frame_id, scale)] = F.grid_sample(inputs[(\"color\", frame_id, source_scale)],\n",
    "                outputs[(\"coords\", frame_id, scale)],\n",
    "                padding_mode=\"border\")\n",
    "\n",
    "    return outputs             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reprojection_loss(pred, target):\n",
    "\n",
    "        l1_loss = (torch.abs(target - pred)).mean(1,True)\n",
    "        \n",
    "        #reprojection_loss = l1_loss\n",
    "\n",
    "        ssim_loss = ssim(pred, target).mean(1, True)\n",
    "        reprojection_loss = 0.85 * ssim_loss + 0.15 * l1_loss\n",
    "\n",
    "        return reprojection_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_loss(disp, img):\n",
    "\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x = grad_disp_x*torch.exp(-grad_img_x)\n",
    "    grad_disp_y = grad_disp_y*torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(inputs, outputs):\n",
    "\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "        frame_id = 's'\n",
    "\n",
    "        for scale in scales:\n",
    "            loss = 0\n",
    "            reprojection_losses = []\n",
    "\n",
    "            source_scale = 0\n",
    "\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, source_scale)]\n",
    "\n",
    "\n",
    "            pred = outputs[(\"color\", frame_id, scale)]\n",
    "            reprojection_losses.append(compute_reprojection_loss(pred, target))\n",
    "\n",
    "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
    "            to_optimise = reprojection_losses\n",
    "            loss = loss + to_optimise.mean()\n",
    "\n",
    "            norm_disp = disp / (disp.mean(2, True).mean(3, True)+ 1e-7)\n",
    "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
    "\n",
    "            loss =  loss + 1e-3* smooth_loss / (2 ** scale)\n",
    "            total_loss = total_loss + loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "\n",
    "        total_loss = total_loss/num_scales\n",
    "        losses[\"loss\"] = total_loss\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_depth_losses(inputs, outputs, losses):\n",
    "\n",
    "\n",
    "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
    "        depth_pred = torch.clamp(F.interpolate(\n",
    "            depth_pred, [375, 1242], mode=\"bilinear\", align_corners=True), 1e-3, 80)\n",
    "        depth_pred = depth_pred.detach()\n",
    "\n",
    "        depth_gt = inputs[\"depth_gt\"]\n",
    "        mask = depth_gt > 0\n",
    "\n",
    "        crop_mask = torch.zeros_like(mask)\n",
    "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * crop_mask\n",
    "\n",
    "        gt = depth_gt[mask]\n",
    "        pred = depth_pred[mask]\n",
    "        pred = pred*(torch.median(gt) / torch.median(pred))\n",
    "\n",
    "        pred = torch.clamp(pred, min=1e-3, max=80)\n",
    "        thresh = torch.max((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).float().mean()\n",
    "        a2 = (thresh < 1.25 ** 2).float().mean()\n",
    "        a3 = (thresh < 1.25 ** 3).float().mean()\n",
    "\n",
    "        rmse = torch.sqrt(((gt - pred)**2).mean())\n",
    "        rmse_log = torch.sqrt(((torch.log(gt) - torch.log(pred))**2).mean())\n",
    "        abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
    "        sq_rel = torch.mean((gt - pred) ** 2 / gt)\n",
    "        \n",
    "        return np.array([abs_rel.item(),sq_rel.item(),rmse.item(),rmse_log.item(),a1.item(),a2.item(),a3.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader,val_loader,num_epochs):\n",
    "              \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            total_loss = 0  \n",
    "\n",
    "            for batch_idx, inputs in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "                break\n",
    "                outputs = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "                losses = compute_losses(inputs, outputs)\n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                optimizer.zero_grad()\n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                del inputs, outputs, losses,features\n",
    "                if batch_idx%100==0 and batch_idx>0:\n",
    "                    print('Batch No: ',batch_idx)\n",
    "                    print('Loss: ',total_loss/(batch_idx+1))\n",
    "  \n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            print('Train Loss at Epoch_{}:'.format(epoch+1),total_loss)\n",
    "            validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,val_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_loss = 0\n",
    "        metrics = np.zeros((7))\n",
    "        for batch_idx, inputs in enumerate(val_loader):\n",
    "\n",
    "            before_op_time = time.time()\n",
    "\n",
    "            for key, ipt in inputs.items():\n",
    "                inputs[key] = ipt.to(device)\n",
    "\n",
    "            features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "            outputs = decoder(features)\n",
    "            outputs = generate_images_pred(inputs, outputs)\n",
    "            losses = compute_losses(inputs, outputs)\n",
    "            total_loss = total_loss + losses['loss'].item()\n",
    "         \n",
    "            metrics = metrics + compute_depth_losses(inputs, outputs, losses)\n",
    "            del inputs, outputs, losses,features\n",
    "         \n",
    "        total_loss = total_loss/len(val_loader)\n",
    "        print('Total Loss (Validation): ',total_loss)\n",
    "        print('Error Metrics: ',metrics/len(val_loader))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 376 and 370 in dimension 2 at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/TH/generic/THTensor.cpp:612\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-641dbae087f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-d0f22fd8d3f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 74, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 376 and 370 in dimension 2 at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/TH/generic/THTensor.cpp:612\n"
     ]
    }
   ],
   "source": [
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 2862) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d78ab0e8e40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-fdcf75940c9c>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(encoder, decoder, val_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mbefore_op_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgotit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 2862) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
