{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT RUN THIS CELL..WORK IN PROGRESS\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  \n",
    "\n",
    "import PIL.Image as pil\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from collections import Counter\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import json\n",
    "\n",
    "from utils import *\n",
    "from kitti_utils import *\n",
    "from layers import *\n",
    "\n",
    "import datasets\n",
    "import networks\n",
    "from IPython import embed\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL..WORK IN PROGRESS\n",
    "\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL..WORK IN PROGRESS\n",
    "\n",
    "class MainDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,data_path,filenames,height,width,frame_idxs,num_scales,is_train=False,img_ext='.jpg'):\n",
    "        super(MainDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.filenames = filenames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_scales = num_scales\n",
    "        self.frame_idxs = frame_idxs\n",
    "        self.is_train = is_train\n",
    "        self.img_ext = img_ext\n",
    "        \n",
    "        self.interp = Image.ANTIALIAS\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            self.brightness = (0.8, 1.2)\n",
    "            self.contrast = (0.8, 1.2)\n",
    "            self.saturation = (0.8, 1.2)\n",
    "            self.hue = (-0.1, 0.1)\n",
    "            transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        except TypeError:\n",
    "            self.brightness = 0.2\n",
    "            self.contrast = 0.2\n",
    "            self.saturation = 0.2\n",
    "            self.hue = 0.1\n",
    "        \n",
    "        \n",
    "        self.resize = {}\n",
    "        for i in range(self.num_scales):\n",
    "            s = 2 ** i\n",
    "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),interpolation=self.interp)\n",
    "            \n",
    "        self.load_depth = self.check_depth()\n",
    "        \n",
    "        self.K = np.array([[0.58, 0, 0.5, 0],\n",
    "                           [0, 1.92, 0.5, 0],\n",
    "                           [0, 0, 1, 0],\n",
    "                           [0, 0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "        self.full_res_shape = (1242, 375)\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def preprocess(self, inputs, color_aug):\n",
    "        \n",
    "        for k in list(inputs):\n",
    "            frame = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                for i in range(self.num_scales):\n",
    "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
    "\n",
    "        for k in list(inputs):\n",
    "            f = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                inputs[(n, im, i)] = self.to_tensor(f)\n",
    "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
    "\n",
    "                    \n",
    "    def check_depth(self):\n",
    "        line = self.filenames[0].split()\n",
    "        scene_name = line[0]\n",
    "        frame_index = int(line[1])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            scene_name,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        return os.path.isfile(velo_filename)\n",
    "    \n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "        \n",
    "        color = self.loader(self.get_image_path(folder, frame_index, side))\n",
    "        if do_flip:\n",
    "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return color\n",
    "    \n",
    "    def get_image_path(self, folder, frame_index, side):\n",
    "        f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\n",
    "        image_path = os.path.join(\n",
    "            self.data_path, folder, \"image_0{}/data\".format(self.side_map[side]), f_str)\n",
    "        return image_path\n",
    "\n",
    "    def get_depth(self, folder, frame_index, side):\n",
    "        calib_path = os.path.join(self.data_path, folder.split(\"/\")[0])\n",
    "\n",
    "        velo_filename = os.path.join(\n",
    "            self.data_path,\n",
    "            folder,\n",
    "            \"velodyne_points/data/{:010d}.bin\".format(int(frame_index)))\n",
    "\n",
    "        depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\n",
    "        depth_gt = skimage.transform.resize(\n",
    "            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\n",
    "\n",
    "\n",
    "        return depth_gt\n",
    "                    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        inputs = {}\n",
    "        \n",
    "        do_color_aug = self.is_train and random.random() > 0.5\n",
    "        do_flip = self.is_train and random.random() > 0.5\n",
    "        \n",
    "        line = self.filenames[index].split()\n",
    "        folder = line[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(line) == 3:\n",
    "            frame_index = int(line[1])\n",
    "            side = line[2]\n",
    "        else:\n",
    "            frame_index = 0\n",
    "            side = None\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            if i == \"s\":\n",
    "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\n",
    "            else:\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n",
    "                \n",
    "        for scale in range(self.num_scales):\n",
    "            K = self.K.copy()\n",
    "\n",
    "            K[0, :] *= self.width // (2 ** scale)\n",
    "            K[1, :] *= self.height // (2 ** scale)\n",
    "\n",
    "            inv_K = np.linalg.pinv(K)\n",
    "\n",
    "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
    "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
    "\n",
    "        if do_color_aug:\n",
    "            color_aug = transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        else:\n",
    "            color_aug = (lambda x: x)\n",
    "            \n",
    "        self.preprocess(inputs,color_aug)\n",
    "        \n",
    "        for i in self.frame_idxs:\n",
    "            del inputs[(\"color\", i, -1)]\n",
    "            #del inputs[(\"color_aug\", i, -1)]\n",
    "            \n",
    "        if self.load_depth:\n",
    "            depth_gt = self.get_depth(folder, frame_index, side)\n",
    "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
    "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
    "            \n",
    "        if \"s\" in self.frame_idxs:\n",
    "            stereo_T = np.eye(4, dtype=np.float32)\n",
    "            baseline_sign =  +1 #-1 if do_flip else 1\n",
    "            side_sign = -1 if side == \"l\" else 1\n",
    "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n",
    "\n",
    "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_velodyne_points(filename):\n",
    "    \"\"\"Load 3D point cloud from KITTI file format\n",
    "    (adapted from https://github.com/hunse/kitti)\n",
    "    \"\"\"\n",
    "    points = np.fromfile(filename, dtype=np.float32).reshape(-1, 4)\n",
    "    points[:, 3] = 1.0  # homogeneous\n",
    "    return points\n",
    "\n",
    "\n",
    "def read_calib_file(path):\n",
    "    \"\"\"Read KITTI calibration file\n",
    "    (from https://github.com/hunse/kitti)\n",
    "    \"\"\"\n",
    "    float_chars = set(\"0123456789.e+- \")\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split(':', 1)\n",
    "            value = value.strip()\n",
    "            data[key] = value\n",
    "            if float_chars.issuperset(value):\n",
    "                # try to cast to float array\n",
    "                try:\n",
    "                    data[key] = np.array(list(map(float, value.split(' '))))\n",
    "                except ValueError:\n",
    "                    # casting error: data[key] already eq. value, so pass\n",
    "                    pass\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def sub2ind(matrixSize, rowSub, colSub):\n",
    "    \"\"\"Convert row, col matrix subscripts to linear indices\n",
    "    \"\"\"\n",
    "    m, n = matrixSize\n",
    "    return rowSub * (n-1) + colSub - 1\n",
    "\n",
    "\n",
    "def generate_depth_map(calib_dir, velo_filename, cam=2, vel_depth=False):\n",
    "    \"\"\"Generate a depth map from velodyne data\n",
    "    \"\"\"\n",
    "    # load calibration files\n",
    "    cam2cam = read_calib_file(os.path.join(calib_dir, 'calib_cam_to_cam.txt'))\n",
    "    velo2cam = read_calib_file(os.path.join(calib_dir, 'calib_velo_to_cam.txt'))\n",
    "    velo2cam = np.hstack((velo2cam['R'].reshape(3, 3), velo2cam['T'][..., np.newaxis]))\n",
    "    velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n",
    "\n",
    "    # get image shape\n",
    "    im_shape = cam2cam[\"S_rect_02\"][::-1].astype(np.int32)\n",
    "\n",
    "    # compute projection matrix velodyne->image plane\n",
    "    R_cam2rect = np.eye(4)\n",
    "    R_cam2rect[:3, :3] = cam2cam['R_rect_00'].reshape(3, 3)\n",
    "    P_rect = cam2cam['P_rect_0'+str(cam)].reshape(3, 4)\n",
    "    P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n",
    "\n",
    "    # load velodyne points and remove all behind image plane (approximation)\n",
    "    # each row of the velodyne data is forward, left, up, reflectance\n",
    "    velo = load_velodyne_points(velo_filename)\n",
    "    velo = velo[velo[:, 0] >= 0, :]\n",
    "\n",
    "    # project the points to the camera\n",
    "    velo_pts_im = np.dot(P_velo2im, velo.T).T\n",
    "    velo_pts_im[:, :2] = velo_pts_im[:, :2] / velo_pts_im[:, 2][..., np.newaxis]\n",
    "\n",
    "    if vel_depth:\n",
    "        velo_pts_im[:, 2] = velo[:, 0]\n",
    "\n",
    "    # check if in bounds\n",
    "    # use minus 1 to get the exact same value as KITTI matlab code\n",
    "    velo_pts_im[:, 0] = np.round(velo_pts_im[:, 0]) - 1\n",
    "    velo_pts_im[:, 1] = np.round(velo_pts_im[:, 1]) - 1\n",
    "    val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n",
    "    val_inds = val_inds & (velo_pts_im[:, 0] < im_shape[1]) & (velo_pts_im[:, 1] < im_shape[0])\n",
    "    velo_pts_im = velo_pts_im[val_inds, :]\n",
    "\n",
    "    # project to image\n",
    "    depth = np.zeros((im_shape[:2]))\n",
    "    depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n",
    "\n",
    "    # find the duplicate points and choose the closest depth\n",
    "    inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n",
    "    dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n",
    "    for dd in dupe_inds:\n",
    "        pts = np.where(inds == dd)[0]\n",
    "        x_loc = int(velo_pts_im[pts[0], 0])\n",
    "        y_loc = int(velo_pts_im[pts[0], 1])\n",
    "        depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n",
    "    depth[depth < 0] = 0\n",
    "\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset Parameters\n",
    "data_path = \"/home/ubuntu/monodepth2/kitti_data\"\n",
    "height = 192\n",
    "width = 640\n",
    "\n",
    "num_scales = 4\n",
    "img_ext = '.jpg'\n",
    "\n",
    "#Loader Parameters\n",
    "batch_size = 12\n",
    "num_workers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_train = \"/home/ubuntu/monodepth2/splits/eigen_zhou/train_files.txt\"\n",
    "f_train = open(fpath_train)\n",
    "train_filenames = f_train.readlines()\n",
    "#train_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_val = \"/home/ubuntu/monodepth2/splits/eigen_zhou/val_files.txt\"\n",
    "f_val = open(fpath_val)\n",
    "val_filenames = f_val.readlines()\n",
    "#val_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_dict = {\"kitti\": datasets.KITTIRAWDataset}\n",
    "datasets_dict = {\"kitti\": MainDataset}\n",
    "dataset = datasets_dict[\"kitti\"]\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSIM(\n",
       "  (mu_x_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (mu_y_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (sig_x_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (sig_y_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (sig_xy_pool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (refl): ReflectionPad2d((1, 1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height = 192\n",
    "width = 640\n",
    "scales = np.arange(4)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "frame_ids = [0,'s']\n",
    "weights_init = 'pretrained'\n",
    "num_scales = len(scales)\n",
    "num_input_frames = len(frame_ids)\n",
    "num_layers = 18\n",
    "learning_rate = 0.001\n",
    "ssim = SSIM()\n",
    "ssim.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = dataset(data_path, train_filenames, height, width,frame_ids,num_scales,is_train=True, img_ext=img_ext)\n",
    "train_loader = DataLoader(train_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = dataset(data_path, val_filenames, height, width,frame_ids, 4, is_train=False, img_ext=img_ext)\n",
    "val_loader = DataLoader(val_dataset, batch_size, True,num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "encoder = networks.ResnetEncoder(num_layers, weights_init == \"pretrained\")\n",
    "params += list(encoder.parameters())\n",
    "\n",
    "decoder = networks.DepthDecoder(encoder.num_ch_enc, scales)\n",
    "params += list(decoder.parameters())\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "optimizer = optim.Adam(params, learning_rate)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 15, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39810 training items and 4424 validation items\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backproject_depth = {}\n",
    "project_3d = {}\n",
    "for scale in scales:\n",
    "    h = height // (2 ** scale)\n",
    "    w = width // (2 ** scale)\n",
    "\n",
    "    backproject_depth[scale] = BackprojectDepth(batch_size, h, w)\n",
    "    backproject_depth[scale].to(device)\n",
    "\n",
    "    project_3d[scale] = Project3D(batch_size, h, w)\n",
    "    project_3d[scale].to(device)\n",
    "print(\"There are {:d} training items and {:d} validation items\\n\".format(len(train_dataset), len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_pred(inputs, outputs):\n",
    "        \n",
    "    frame_id = 's'\n",
    "    for scale in scales:\n",
    "        disp = outputs[(\"disp\", scale)]\n",
    "\n",
    "        disp = F.interpolate(disp, [height, width], mode=\"bilinear\", align_corners=True)\n",
    "        source_scale = 0\n",
    "\n",
    "        _, depth = disp_to_depth(disp, 0.1, 100)\n",
    "\n",
    "        outputs[(\"depth\", 0, scale)] = depth\n",
    "\n",
    "        T = inputs[\"stereo_T\"]\n",
    "\n",
    "        cam_points = backproject_depth[source_scale](depth, inputs[(\"inv_K\", source_scale)])\n",
    "        pix_coords = project_3d[source_scale](cam_points, inputs[(\"K\", source_scale)], T)\n",
    "        outputs[(\"coords\", frame_id, scale)] = pix_coords\n",
    "        outputs[(\"color\", frame_id, scale)] = F.grid_sample(inputs[(\"color\", frame_id, source_scale)],\n",
    "                outputs[(\"coords\", frame_id, scale)],\n",
    "                padding_mode=\"border\")\n",
    "\n",
    "    return outputs             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reprojection_loss(pred, target):\n",
    "\n",
    "        l1_loss = (torch.abs(target - pred)).mean(1,True)\n",
    "        \n",
    "        #reprojection_loss = l1_loss\n",
    "\n",
    "        ssim_loss = ssim(pred, target).mean(1, True)\n",
    "        reprojection_loss = 0.85 * ssim_loss + 0.15 * l1_loss\n",
    "\n",
    "        return reprojection_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_loss(disp, img):\n",
    "\n",
    "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
    "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
    "\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
    "\n",
    "    grad_disp_x = grad_disp_x*torch.exp(-grad_img_x)\n",
    "    grad_disp_y = grad_disp_y*torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_disp_x.mean() + grad_disp_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(inputs, outputs):\n",
    "\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "        frame_id = 's'\n",
    "\n",
    "        for scale in scales:\n",
    "            loss = 0\n",
    "            reprojection_losses = []\n",
    "\n",
    "            source_scale = 0\n",
    "\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, source_scale)]\n",
    "\n",
    "\n",
    "            pred = outputs[(\"color\", frame_id, scale)]\n",
    "            reprojection_losses.append(compute_reprojection_loss(pred, target))\n",
    "\n",
    "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
    "            to_optimise = reprojection_losses\n",
    "            loss = loss + to_optimise.mean()\n",
    "\n",
    "            norm_disp = disp / (disp.mean(2, True).mean(3, True)+ 1e-7)\n",
    "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
    "\n",
    "            loss =  loss + 1e-3* smooth_loss / (2 ** scale)\n",
    "            total_loss = total_loss + loss\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "\n",
    "        total_loss = total_loss/num_scales\n",
    "        losses[\"loss\"] = total_loss\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_depth_losses(inputs, outputs, losses):\n",
    "\n",
    "\n",
    "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
    "        depth_pred = torch.clamp(F.interpolate(\n",
    "            depth_pred, [375, 1242], mode=\"bilinear\", align_corners=True), 1e-3, 80)\n",
    "        depth_pred = depth_pred.detach()\n",
    "\n",
    "        depth_gt = inputs[\"depth_gt\"]\n",
    "        mask = depth_gt > 0\n",
    "\n",
    "        crop_mask = torch.zeros_like(mask)\n",
    "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
    "        mask = mask * crop_mask\n",
    "\n",
    "        gt = depth_gt[mask]\n",
    "        pred = depth_pred[mask]\n",
    "        pred = pred*(torch.median(gt) / torch.median(pred))\n",
    "\n",
    "        pred = torch.clamp(pred, min=1e-3, max=80)\n",
    "        thresh = torch.max((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).float().mean()\n",
    "        a2 = (thresh < 1.25 ** 2).float().mean()\n",
    "        a3 = (thresh < 1.25 ** 3).float().mean()\n",
    "\n",
    "        rmse = torch.sqrt(((gt - pred)**2).mean())\n",
    "        rmse_log = torch.sqrt(((torch.log(gt) - torch.log(pred))**2).mean())\n",
    "        abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
    "        sq_rel = torch.mean((gt - pred) ** 2 / gt)\n",
    "        \n",
    "        return np.array([abs_rel.item(),sq_rel.item(),rmse.item(),rmse_log.item(),a1.item(),a2.item(),a3.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader,val_loader,num_epochs):\n",
    "              \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            total_loss = 0  \n",
    "\n",
    "            for batch_idx, inputs in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                for key, ipt in inputs.items():\n",
    "                    inputs[key] = ipt.to(device)\n",
    "\n",
    "                features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "                outputs = decoder(features)\n",
    "                break\n",
    "                outputs = generate_images_pred(inputs, outputs)\n",
    "                \n",
    "                losses = compute_losses(inputs, outputs)\n",
    "                total_loss = total_loss + losses['loss'].item()\n",
    "                optimizer.zero_grad()\n",
    "                losses[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                del inputs, outputs, losses,features\n",
    "                if batch_idx%100==0 and batch_idx>0:\n",
    "                    print('Batch No: ',batch_idx)\n",
    "                    print('Loss: ',total_loss/(batch_idx+1))\n",
    "  \n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            print('Train Loss at Epoch_{}:'.format(epoch+1),total_loss)\n",
    "            validation(encoder,decoder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,val_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_loss = 0\n",
    "        metrics = np.zeros((7))\n",
    "        for batch_idx, inputs in enumerate(val_loader):\n",
    "\n",
    "            before_op_time = time.time()\n",
    "\n",
    "            for key, ipt in inputs.items():\n",
    "                inputs[key] = ipt.to(device)\n",
    "\n",
    "            features = encoder(inputs[\"color_aug\", 0, 0])\n",
    "            outputs = decoder(features)\n",
    "            outputs = generate_images_pred(inputs, outputs)\n",
    "            losses = compute_losses(inputs, outputs)\n",
    "            total_loss = total_loss + losses['loss'].item()\n",
    "         \n",
    "            metrics = metrics + compute_depth_losses(inputs, outputs, losses)\n",
    "            del inputs, outputs, losses,features\n",
    "         \n",
    "        total_loss = total_loss/len(val_loader)\n",
    "        print('Total Loss (Validation): ',total_loss)\n",
    "        print('Error Metrics: ',metrics/len(val_loader))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(encoder,decoder,train_loader,val_loader,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss (Validation):  0.1072427833425826\n",
      "Error Metrics:  [ 0.48219792  4.97419271 12.19717556  0.61915746  0.27533514  0.53179234\n",
      "  0.76004968]\n"
     ]
    }
   ],
   "source": [
    "validation(encoder,decoder,val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
